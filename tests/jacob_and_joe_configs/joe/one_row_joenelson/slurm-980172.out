/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/albumentations/check_version.py:51: UserWarning: Error fetching version info <urlopen error [Errno 101] Network is unreachable>
  data = fetch_version_info()
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epochs:   0%|          | 0/200 [00:00<?, ?epoch/s]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:32<26:24, 32.33s/batch][A
Batches:   4%|▍         | 2/50 [00:35<12:06, 15.14s/batch][A
Batches:   6%|▌         | 3/50 [00:38<07:28,  9.53s/batch][A
Batches:   8%|▊         | 4/50 [00:41<05:18,  6.93s/batch][A
Batches:  10%|█         | 5/50 [00:44<04:08,  5.52s/batch][A
Batches:  12%|█▏        | 6/50 [00:47<03:23,  4.64s/batch][A
Batches:  14%|█▍        | 7/50 [00:50<02:54,  4.05s/batch][A
Batches:  16%|█▌        | 8/50 [00:52<02:34,  3.67s/batch][A
Batches:  18%|█▊        | 9/50 [00:55<02:22,  3.47s/batch][A
Batches:  20%|██        | 10/50 [00:58<02:10,  3.27s/batch][A
Batches:  22%|██▏       | 11/50 [01:01<02:03,  3.17s/batch][A
Batches:  24%|██▍       | 12/50 [01:04<01:57,  3.08s/batch][A
Batches:  26%|██▌       | 13/50 [01:07<01:53,  3.07s/batch][A
Batches:  28%|██▊       | 14/50 [01:10<01:47,  3.00s/batch][A
Batches:  30%|███       | 15/50 [01:13<01:43,  2.96s/batch][A
Batches:  32%|███▏      | 16/50 [01:16<01:40,  2.95s/batch][A
Batches:  34%|███▍      | 17/50 [01:19<01:37,  2.95s/batch][A
Batches:  36%|███▌      | 18/50 [01:22<01:33,  2.93s/batch][A
Batches:  38%|███▊      | 19/50 [01:24<01:30,  2.91s/batch][A
Batches:  40%|████      | 20/50 [01:27<01:26,  2.89s/batch][A
Batches:  42%|████▏     | 21/50 [01:30<01:24,  2.92s/batch][A
Batches:  44%|████▍     | 22/50 [01:33<01:20,  2.88s/batch][A
Batches:  46%|████▌     | 23/50 [01:36<01:17,  2.88s/batch][A
Batches:  48%|████▊     | 24/50 [01:39<01:14,  2.88s/batch][A
Batches:  50%|█████     | 25/50 [01:42<01:13,  2.93s/batch][A
Batches:  52%|█████▏    | 26/50 [01:45<01:10,  2.92s/batch][A
Batches:  54%|█████▍    | 27/50 [01:48<01:07,  2.92s/batch][A
Batches:  56%|█████▌    | 28/50 [01:51<01:03,  2.89s/batch][A
Batches:  58%|█████▊    | 29/50 [01:53<01:01,  2.92s/batch][A
Batches:  60%|██████    | 30/50 [01:56<00:57,  2.90s/batch][A
Batches:  62%|██████▏   | 31/50 [01:59<00:54,  2.89s/batch][A
Batches:  64%|██████▍   | 32/50 [02:02<00:52,  2.91s/batch][A
Batches:  66%|██████▌   | 33/50 [02:05<00:49,  2.90s/batch][A
Batches:  68%|██████▊   | 34/50 [02:08<00:46,  2.90s/batch][A
Batches:  70%|███████   | 35/50 [02:11<00:43,  2.90s/batch][A
Batches:  72%|███████▏  | 36/50 [02:14<00:41,  2.93s/batch][A
Batches:  74%|███████▍  | 37/50 [02:17<00:37,  2.92s/batch][A
Batches:  76%|███████▌  | 38/50 [02:20<00:34,  2.89s/batch][A
Batches:  78%|███████▊  | 39/50 [02:22<00:31,  2.88s/batch][A
Batches:  80%|████████  | 40/50 [02:25<00:29,  2.91s/batch][A
Batches:  82%|████████▏ | 41/50 [02:28<00:26,  2.89s/batch][A
Batches:  84%|████████▍ | 42/50 [02:31<00:23,  2.88s/batch][A
Batches:  86%|████████▌ | 43/50 [02:34<00:20,  2.88s/batch][A
Batches:  88%|████████▊ | 44/50 [02:37<00:17,  2.89s/batch][A
Batches:  90%|█████████ | 45/50 [02:40<00:14,  2.89s/batch][A
Batches:  92%|█████████▏| 46/50 [02:43<00:11,  2.89s/batch][A
Batches:  94%|█████████▍| 47/50 [02:45<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:48<00:05,  2.90s/batch][A
Batches:  98%|█████████▊| 49/50 [02:51<00:02,  2.88s/batch][A
Batches: 100%|██████████| 50/50 [02:54<00:00,  2.86s/batch][A
                                                           [AEpochs:   0%|          | 1/200 [02:59<9:54:10, 179.15s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:17,  2.81s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.89s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.86s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:07,  2.89s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.87s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:58,  2.88s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.89s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.88s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.88s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:48,  2.94s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:45,  2.94s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:42,  2.92s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:38,  2.91s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:37,  2.95s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:33,  2.93s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:29,  2.90s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:27,  2.91s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:25,  2.93s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:21,  2.90s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.91s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:15,  2.89s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:12,  2.90s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:09,  2.89s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.88s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:23<01:00,  2.90s/batch][A
Batches:  60%|██████    | 30/50 [01:26<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:29<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:32<00:51,  2.88s/batch][A
Batches:  66%|██████▌   | 33/50 [01:35<00:49,  2.90s/batch][A
Batches:  68%|██████▊   | 34/50 [01:38<00:45,  2.87s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:40,  2.90s/batch][A
Batches:  74%|███████▍  | 37/50 [01:46<00:37,  2.87s/batch][A
Batches:  76%|███████▌  | 38/50 [01:49<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:52<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:55<00:28,  2.89s/batch][A
Batches:  82%|████████▏ | 41/50 [01:58<00:25,  2.88s/batch][A
Batches:  84%|████████▍ | 42/50 [02:01<00:23,  2.88s/batch][A
Batches:  86%|████████▌ | 43/50 [02:04<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:07<00:17,  2.87s/batch][A
Batches:  90%|█████████ | 45/50 [02:09<00:14,  2.88s/batch][A
Batches:  92%|█████████▏| 46/50 [02:12<00:11,  2.89s/batch][A
Batches:  94%|█████████▍| 47/50 [02:15<00:08,  2.88s/batch][A
Batches:  96%|█████████▌| 48/50 [02:18<00:05,  2.92s/batch][A
Batches:  98%|█████████▊| 49/50 [02:21<00:02,  2.91s/batch][A
Batches: 100%|██████████| 50/50 [02:24<00:00,  2.91s/batch][A
                                                           [AEpochs:   1%|          | 2/200 [05:27<8:50:55, 160.89s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:23,  2.92s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:23,  3.00s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:17,  2.92s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:13,  2.91s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.88s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:07,  2.91s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.88s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:00,  2.86s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.88s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.88s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:50,  2.90s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:48,  2.94s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:45,  2.93s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:42,  2.93s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:38,  2.90s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:37,  2.95s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:33,  2.94s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:30,  2.92s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:27,  2.92s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:25,  2.94s/batch][A
Batches:  44%|████▍     | 22/50 [01:04<01:21,  2.92s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.92s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:15,  2.92s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:13,  2.95s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:10,  2.94s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:07,  2.93s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:04,  2.92s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:02,  2.96s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:58,  2.93s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:55,  2.93s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:52,  2.93s/batch][A
Batches:  66%|██████▌   | 33/50 [01:36<00:50,  2.95s/batch][A
Batches:  68%|██████▊   | 34/50 [01:39<00:47,  2.94s/batch][A
Batches:  70%|███████   | 35/50 [01:42<00:44,  2.95s/batch][A
Batches:  72%|███████▏  | 36/50 [01:45<00:41,  2.95s/batch][A
Batches:  74%|███████▍  | 37/50 [01:48<00:38,  2.97s/batch][A
Batches:  76%|███████▌  | 38/50 [01:51<00:35,  2.96s/batch][A
Batches:  78%|███████▊  | 39/50 [01:54<00:32,  2.95s/batch][A
Batches:  80%|████████  | 40/50 [01:57<00:29,  2.95s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:26,  2.94s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:23,  2.94s/batch][A
Batches:  86%|████████▌ | 43/50 [02:05<00:20,  2.93s/batch][A
Batches:  88%|████████▊ | 44/50 [02:08<00:17,  2.96s/batch][A
Batches:  90%|█████████ | 45/50 [02:11<00:14,  2.94s/batch][A
Batches:  92%|█████████▏| 46/50 [02:14<00:11,  2.91s/batch][A
Batches:  94%|█████████▍| 47/50 [02:17<00:08,  2.92s/batch][A
Batches:  96%|█████████▌| 48/50 [02:20<00:05,  2.93s/batch][A
Batches:  98%|█████████▊| 49/50 [02:23<00:02,  2.93s/batch][A
Batches: 100%|██████████| 50/50 [02:26<00:00,  2.92s/batch][A
                                                           [AEpochs:   2%|▏         | 3/200 [07:57<8:32:07, 155.98s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:21,  2.89s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:21,  2.94s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.90s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:13,  2.89s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:11,  2.91s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:07,  2.91s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.88s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:00,  2.87s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<02:00,  2.93s/batch][A
Batches:  20%|██        | 10/50 [00:29<01:57,  2.94s/batch][A
Batches:  22%|██▏       | 11/50 [00:32<01:54,  2.93s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:50,  2.91s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:48,  2.94s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:45,  2.93s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:41,  2.91s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:38,  2.91s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:37,  2.95s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:33,  2.92s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:30,  2.90s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:26,  2.88s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:24,  2.91s/batch][A
Batches:  44%|████▍     | 22/50 [01:04<01:21,  2.91s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.90s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:15,  2.92s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:13,  2.93s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:09,  2.90s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.90s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:03,  2.90s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:01,  2.94s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:58,  2.91s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:51,  2.87s/batch][A
Batches:  66%|██████▌   | 33/50 [01:35<00:49,  2.90s/batch][A
Batches:  68%|██████▊   | 34/50 [01:38<00:46,  2.90s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.89s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:41,  2.93s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.90s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:34,  2.90s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:32,  2.91s/batch][A
Batches:  80%|████████  | 40/50 [01:56<00:29,  2.95s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:26,  2.95s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:23,  2.92s/batch][A
Batches:  86%|████████▌ | 43/50 [02:05<00:20,  2.90s/batch][A
Batches:  88%|████████▊ | 44/50 [02:08<00:17,  2.94s/batch][A
Batches:  90%|█████████ | 45/50 [02:11<00:14,  2.92s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.92s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.91s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.94s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.92s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.90s/batch][A
                                                           [AEpochs:   2%|▏         | 4/200 [10:26<8:20:56, 153.35s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:23,  2.93s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:22,  2.97s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.91s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:14,  2.92s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:12,  2.95s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:08,  2.93s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:04,  2.89s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:00,  2.86s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.89s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.89s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:47,  2.90s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.89s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:41,  2.89s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:37,  2.88s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:36,  2.92s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:33,  2.93s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:30,  2.92s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:27,  2.92s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:25,  2.96s/batch][A
Batches:  44%|████▍     | 22/50 [01:04<01:22,  2.96s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:19,  2.94s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:16,  2.93s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:13,  2.95s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:10,  2.92s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:07,  2.91s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:04,  2.95s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:01,  2.94s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:58,  2.94s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:55,  2.92s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:52,  2.94s/batch][A
Batches:  66%|██████▌   | 33/50 [01:36<00:49,  2.90s/batch][A
Batches:  68%|██████▊   | 34/50 [01:39<00:46,  2.90s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.88s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:40,  2.91s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.90s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:34,  2.88s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:56<00:29,  2.91s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:26,  2.91s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:23,  2.91s/batch][A
Batches:  86%|████████▌ | 43/50 [02:05<00:20,  2.91s/batch][A
Batches:  88%|████████▊ | 44/50 [02:08<00:17,  2.92s/batch][A
Batches:  90%|█████████ | 45/50 [02:11<00:14,  2.91s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.90s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.91s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.88s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.88s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.85s/batch][A
                                                           [AEpochs:   2%|▎         | 5/200 [12:55<8:13:15, 151.77s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:25,  2.97s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.88s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.88s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.86s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:10,  2.90s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:04,  2.89s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:03,  2.94s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<02:00,  2.93s/batch][A
Batches:  20%|██        | 10/50 [00:29<01:56,  2.91s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.88s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:48,  2.92s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:45,  2.93s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:42,  2.92s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:40,  2.95s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:37,  2.94s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:34,  2.94s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:30,  2.92s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:28,  2.94s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:24,  2.91s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:20,  2.89s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.89s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:15,  2.90s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:12,  2.90s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.87s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:03,  2.89s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:26<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:29<00:54,  2.88s/batch][A
Batches:  64%|██████▍   | 32/50 [01:32<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:35<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:38<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.91s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:39,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:46<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:49<00:34,  2.88s/batch][A
Batches:  78%|███████▊  | 39/50 [01:52<00:32,  2.93s/batch][A
Batches:  80%|████████  | 40/50 [01:55<00:29,  2.91s/batch][A
Batches:  82%|████████▏ | 41/50 [01:58<00:26,  2.89s/batch][A
Batches:  84%|████████▍ | 42/50 [02:01<00:23,  2.91s/batch][A
Batches:  86%|████████▌ | 43/50 [02:04<00:20,  2.90s/batch][A
Batches:  88%|████████▊ | 44/50 [02:07<00:17,  2.91s/batch][A
Batches:  90%|█████████ | 45/50 [02:10<00:14,  2.89s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.89s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.92s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.91s/batch][A
Batches:  98%|█████████▊| 49/50 [02:21<00:02,  2.91s/batch][A
Batches: 100%|██████████| 50/50 [02:24<00:00,  2.94s/batch][A
                                                           [AEpochs:   3%|▎         | 6/200 [15:24<8:07:10, 150.67s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:20,  2.87s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:19,  2.90s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.89s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:15,  2.95s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:11,  2.93s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:09,  2.95s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:06,  2.95s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:04,  2.96s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<02:00,  2.94s/batch][A
Batches:  20%|██        | 10/50 [00:29<01:57,  2.93s/batch][A
Batches:  22%|██▏       | 11/50 [00:32<01:54,  2.94s/batch][A
Batches:  24%|██▍       | 12/50 [00:35<01:52,  2.97s/batch][A
Batches:  26%|██▌       | 13/50 [00:38<01:49,  2.96s/batch][A
Batches:  28%|██▊       | 14/50 [00:41<01:46,  2.95s/batch][A
Batches:  30%|███       | 15/50 [00:44<01:44,  2.97s/batch][A
Batches:  32%|███▏      | 16/50 [00:47<01:39,  2.94s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:36,  2.92s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:33,  2.91s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:31,  2.94s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:27,  2.93s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:24,  2.92s/batch][A
Batches:  44%|████▍     | 22/50 [01:04<01:21,  2.91s/batch][A
Batches:  46%|████▌     | 23/50 [01:07<01:18,  2.92s/batch][A
Batches:  48%|████▊     | 24/50 [01:10<01:15,  2.91s/batch][A
Batches:  50%|█████     | 25/50 [01:13<01:12,  2.89s/batch][A
Batches:  52%|█████▏    | 26/50 [01:16<01:09,  2.89s/batch][A
Batches:  54%|█████▍    | 27/50 [01:19<01:07,  2.93s/batch][A
Batches:  56%|█████▌    | 28/50 [01:22<01:04,  2.94s/batch][A
Batches:  58%|█████▊    | 29/50 [01:25<01:01,  2.94s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:58,  2.93s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:55,  2.94s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:52,  2.91s/batch][A
Batches:  66%|██████▌   | 33/50 [01:36<00:49,  2.91s/batch][A
Batches:  68%|██████▊   | 34/50 [01:39<00:46,  2.89s/batch][A
Batches:  70%|███████   | 35/50 [01:42<00:43,  2.92s/batch][A
Batches:  72%|███████▏  | 36/50 [01:45<00:40,  2.89s/batch][A
Batches:  74%|███████▍  | 37/50 [01:48<00:37,  2.89s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:31,  2.88s/batch][A
Batches:  80%|████████  | 40/50 [01:56<00:28,  2.89s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:25,  2.88s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:05<00:20,  2.90s/batch][A
Batches:  88%|████████▊ | 44/50 [02:08<00:17,  2.89s/batch][A
Batches:  90%|█████████ | 45/50 [02:11<00:14,  2.88s/batch][A
Batches:  92%|█████████▏| 46/50 [02:14<00:11,  2.89s/batch][A
Batches:  94%|█████████▍| 47/50 [02:17<00:08,  2.92s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.90s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.90s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.92s/batch][A
                                                           [AEpochs:   4%|▎         | 7/200 [17:53<8:03:38, 150.35s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:22,  2.92s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.87s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.84s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.89s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:10,  2.89s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.88s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:01,  2.90s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:58,  2.90s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:56,  2.92s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:55,  2.96s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:51,  2.93s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:48,  2.92s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:44,  2.90s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:42,  2.93s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:38,  2.91s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:35,  2.90s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:32,  2.89s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:30,  2.93s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:27,  2.92s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:24,  2.91s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:21,  2.91s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.91s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:16,  2.93s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:13,  2.92s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:10,  2.92s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:07,  2.94s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:03,  2.91s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:01,  2.91s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:57,  2.88s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:55,  2.92s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:52,  2.93s/batch][A
Batches:  66%|██████▌   | 33/50 [01:36<00:49,  2.92s/batch][A
Batches:  68%|██████▊   | 34/50 [01:38<00:46,  2.91s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.93s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:40,  2.93s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.90s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:35,  2.94s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:32,  2.93s/batch][A
Batches:  80%|████████  | 40/50 [01:56<00:29,  2.91s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:26,  2.89s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:23,  2.90s/batch][A
Batches:  86%|████████▌ | 43/50 [02:05<00:20,  2.91s/batch][A
Batches:  88%|████████▊ | 44/50 [02:08<00:17,  2.91s/batch][A
Batches:  90%|█████████ | 45/50 [02:10<00:14,  2.90s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.93s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.93s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.89s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.91s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.95s/batch][A
                                                           [AEpochs:   4%|▍         | 8/200 [20:23<8:00:33, 150.17s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:23,  2.92s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:20,  2.92s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.90s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:13,  2.91s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:10,  2.90s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.89s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:04,  2.89s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:01,  2.90s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<01:58,  2.90s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.88s/batch][A
Batches:  22%|██▏       | 11/50 [00:32<01:55,  2.95s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:50,  2.92s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.89s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:41,  2.89s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:38,  2.89s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:35,  2.89s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:32,  2.90s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:31,  2.94s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:28,  2.94s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:25,  2.94s/batch][A
Batches:  44%|████▍     | 22/50 [01:04<01:22,  2.96s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:19,  2.93s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:16,  2.93s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:12,  2.91s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:10,  2.93s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.90s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:03,  2.91s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:01,  2.91s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:59,  2.95s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:55,  2.94s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:52,  2.92s/batch][A
Batches:  66%|██████▌   | 33/50 [01:36<00:49,  2.91s/batch][A
Batches:  68%|██████▊   | 34/50 [01:39<00:47,  2.95s/batch][A
Batches:  70%|███████   | 35/50 [01:42<00:44,  2.95s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:41,  2.93s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.89s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:34,  2.91s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:31,  2.89s/batch][A
Batches:  80%|████████  | 40/50 [01:56<00:29,  2.91s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:26,  2.93s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:23,  2.91s/batch][A
Batches:  86%|████████▌ | 43/50 [02:05<00:20,  2.90s/batch][A
Batches:  88%|████████▊ | 44/50 [02:08<00:17,  2.88s/batch][A
Batches:  90%|█████████ | 45/50 [02:11<00:14,  2.91s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.90s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.90s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.88s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.92s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.88s/batch][A
                                                           [AEpochs:   4%|▍         | 9/200 [22:52<7:57:08, 149.89s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.85s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.90s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.89s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.89s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:09,  2.95s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:05,  2.93s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:01,  2.90s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<02:00,  2.93s/batch][A
Batches:  20%|██        | 10/50 [00:29<01:56,  2.91s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:53,  2.91s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.89s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:48,  2.94s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:45,  2.93s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:42,  2.93s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:39,  2.93s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:37,  2.96s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:34,  2.95s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:30,  2.91s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:27,  2.91s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:24,  2.91s/batch][A
Batches:  44%|████▍     | 22/50 [01:04<01:21,  2.91s/batch][A
Batches:  46%|████▌     | 23/50 [01:07<01:18,  2.92s/batch][A
Batches:  48%|████▊     | 24/50 [01:10<01:16,  2.93s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:13,  2.93s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:10,  2.92s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.88s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:03,  2.90s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:01,  2.91s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:58,  2.91s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:55,  2.92s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:52,  2.90s/batch][A
Batches:  66%|██████▌   | 33/50 [01:36<00:49,  2.92s/batch][A
Batches:  68%|██████▊   | 34/50 [01:39<00:46,  2.90s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.89s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:40,  2.89s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.91s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:34,  2.90s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:31,  2.88s/batch][A
Batches:  80%|████████  | 40/50 [01:56<00:29,  2.91s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:25,  2.88s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:23,  2.90s/batch][A
Batches:  86%|████████▌ | 43/50 [02:05<00:20,  2.89s/batch][A
Batches:  88%|████████▊ | 44/50 [02:08<00:17,  2.93s/batch][A
Batches:  90%|█████████ | 45/50 [02:11<00:14,  2.93s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.90s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.89s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.90s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.89s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.88s/batch][A
                                                           [AEpochs:   5%|▌         | 10/200 [25:22<7:54:05, 149.71s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.85s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:22,  2.96s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.88s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.88s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.88s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:08,  2.91s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:04,  2.90s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:02,  2.92s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<02:01,  2.96s/batch][A
Batches:  20%|██        | 10/50 [00:29<01:57,  2.95s/batch][A
Batches:  22%|██▏       | 11/50 [00:32<01:54,  2.94s/batch][A
Batches:  24%|██▍       | 12/50 [00:35<01:51,  2.93s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:47,  2.91s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:45,  2.93s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:42,  2.92s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:38,  2.89s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:36,  2.93s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:32,  2.90s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:29,  2.89s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:26,  2.89s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:24,  2.92s/batch][A
Batches:  44%|████▍     | 22/50 [01:04<01:20,  2.89s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.90s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:15,  2.92s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:12,  2.92s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:09,  2.89s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.90s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:04,  2.92s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:01,  2.91s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:57,  2.90s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:52,  2.90s/batch][A
Batches:  66%|██████▌   | 33/50 [01:35<00:48,  2.88s/batch][A
Batches:  68%|██████▊   | 34/50 [01:38<00:46,  2.89s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.88s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:40,  2.92s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.91s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:34,  2.90s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:31,  2.89s/batch][A
Batches:  80%|████████  | 40/50 [01:56<00:29,  2.92s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:26,  2.91s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:23,  2.90s/batch][A
Batches:  86%|████████▌ | 43/50 [02:04<00:20,  2.88s/batch][A
Batches:  88%|████████▊ | 44/50 [02:07<00:17,  2.91s/batch][A
Batches:  90%|█████████ | 45/50 [02:10<00:14,  2.89s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.90s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.89s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.94s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.94s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.93s/batch][A
                                                           [AEpochs:   6%|▌         | 11/200 [27:51<7:51:28, 149.67s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:25,  2.96s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:20,  2.93s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:17,  2.93s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.88s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:11,  2.92s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:07,  2.89s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:04,  2.89s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<01:59,  2.86s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<01:59,  2.91s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.88s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.90s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:50,  2.92s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:47,  2.92s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:44,  2.91s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:42,  2.92s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:39,  2.92s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:37,  2.94s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:33,  2.93s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:29,  2.90s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:27,  2.92s/batch][A
Batches:  42%|████▏     | 21/50 [01:01<01:24,  2.91s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:21,  2.90s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.91s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:16,  2.95s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:13,  2.93s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:09,  2.91s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.91s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:05,  2.95s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:02,  2.96s/batch][A
Batches:  60%|██████    | 30/50 [01:27<00:58,  2.92s/batch][A
Batches:  62%|██████▏   | 31/50 [01:30<00:55,  2.94s/batch][A
Batches:  64%|██████▍   | 32/50 [01:33<00:52,  2.91s/batch][A
Batches:  66%|██████▌   | 33/50 [01:36<00:49,  2.90s/batch][A
Batches:  68%|██████▊   | 34/50 [01:38<00:46,  2.89s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.88s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:40,  2.91s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.90s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:31,  2.89s/batch][A
Batches:  80%|████████  | 40/50 [01:56<00:28,  2.89s/batch][A
Batches:  82%|████████▏ | 41/50 [01:59<00:25,  2.87s/batch][A
Batches:  84%|████████▍ | 42/50 [02:02<00:23,  2.89s/batch][A
Batches:  86%|████████▌ | 43/50 [02:04<00:20,  2.89s/batch][A
Batches:  88%|████████▊ | 44/50 [02:07<00:17,  2.91s/batch][A
Batches:  90%|█████████ | 45/50 [02:10<00:14,  2.90s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.88s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.93s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.91s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.89s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.89s/batch][A
                                                           [AEpochs:   6%|▌         | 12/200 [30:20<7:48:06, 149.40s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:26,  3.00s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:20,  2.93s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.89s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:10,  2.91s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.87s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:01,  2.90s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.89s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.89s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:40,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:38,  2.89s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:35,  2.89s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.89s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:27,  2.91s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.88s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:21,  2.90s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.92s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:15,  2.91s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:12,  2.89s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:09,  2.89s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.91s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:04,  2.91s/batch][A
Batches:  58%|█████▊    | 29/50 [01:23<01:00,  2.90s/batch][A
Batches:  60%|██████    | 30/50 [01:26<00:57,  2.88s/batch][A
Batches:  62%|██████▏   | 31/50 [01:29<00:55,  2.92s/batch][A
Batches:  64%|██████▍   | 32/50 [01:32<00:52,  2.90s/batch][A
Batches:  66%|██████▌   | 33/50 [01:35<00:49,  2.90s/batch][A
Batches:  68%|██████▊   | 34/50 [01:38<00:46,  2.92s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.92s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:40,  2.92s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.91s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:35,  2.93s/batch][A
Batches:  78%|███████▊  | 39/50 [01:52<00:31,  2.91s/batch][A
Batches:  80%|████████  | 40/50 [01:55<00:29,  2.91s/batch][A
Batches:  82%|████████▏ | 41/50 [01:58<00:25,  2.88s/batch][A
Batches:  84%|████████▍ | 42/50 [02:01<00:23,  2.92s/batch][A
Batches:  86%|████████▌ | 43/50 [02:04<00:20,  2.91s/batch][A
Batches:  88%|████████▊ | 44/50 [02:07<00:17,  2.92s/batch][A
Batches:  90%|█████████ | 45/50 [02:10<00:14,  2.88s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.91s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.89s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.90s/batch][A
Batches:  98%|█████████▊| 49/50 [02:21<00:02,  2.88s/batch][A
Batches: 100%|██████████| 50/50 [02:24<00:00,  2.91s/batch][A
                                                           [AEpochs:   6%|▋         | 13/200 [32:49<7:45:25, 149.33s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:20,  2.87s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:19,  2.90s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.88s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:15,  2.94s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:10,  2.90s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:07,  2.90s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.88s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:02,  2.93s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<01:59,  2.92s/batch][A
Batches:  20%|██        | 10/50 [00:29<01:56,  2.91s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.88s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:50,  2.91s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:47,  2.91s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:44,  2.90s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:42,  2.92s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:39,  2.92s/batch][A
Batches:  34%|███▍      | 17/50 [00:49<01:35,  2.89s/batch][A
Batches:  36%|███▌      | 18/50 [00:52<01:32,  2.89s/batch][A
Batches:  38%|███▊      | 19/50 [00:55<01:30,  2.92s/batch][A
Batches:  40%|████      | 20/50 [00:58<01:26,  2.90s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:24,  2.90s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:21,  2.91s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.92s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:15,  2.89s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:11,  2.87s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:18<01:06,  2.90s/batch][A
Batches:  56%|█████▌    | 28/50 [01:21<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:24<01:00,  2.88s/batch][A
Batches:  60%|██████    | 30/50 [01:26<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:29<00:55,  2.92s/batch][A
Batches:  64%|██████▍   | 32/50 [01:32<00:52,  2.91s/batch][A
Batches:  66%|██████▌   | 33/50 [01:35<00:49,  2.90s/batch][A
Batches:  68%|██████▊   | 34/50 [01:38<00:46,  2.92s/batch][A
Batches:  70%|███████   | 35/50 [01:41<00:43,  2.92s/batch][A
Batches:  72%|███████▏  | 36/50 [01:44<00:40,  2.91s/batch][A
Batches:  74%|███████▍  | 37/50 [01:47<00:37,  2.88s/batch][A
Batches:  76%|███████▌  | 38/50 [01:50<00:35,  2.93s/batch][A
Batches:  78%|███████▊  | 39/50 [01:53<00:31,  2.90s/batch][A
Batches:  80%|████████  | 40/50 [01:55<00:28,  2.88s/batch][A
Batches:  82%|████████▏ | 41/50 [01:58<00:25,  2.88s/batch][A
Batches:  84%|████████▍ | 42/50 [02:01<00:23,  2.92s/batch][A
Batches:  86%|████████▌ | 43/50 [02:04<00:20,  2.92s/batch][A
Batches:  88%|████████▊ | 44/50 [02:07<00:17,  2.88s/batch][A
Batches:  90%|█████████ | 45/50 [02:10<00:14,  2.88s/batch][A
Batches:  92%|█████████▏| 46/50 [02:13<00:11,  2.94s/batch][A
Batches:  94%|█████████▍| 47/50 [02:16<00:08,  2.91s/batch][A
Batches:  96%|█████████▌| 48/50 [02:19<00:05,  2.90s/batch][A
Batches:  98%|█████████▊| 49/50 [02:22<00:02,  2.94s/batch][A
Batches: 100%|██████████| 50/50 [02:25<00:00,  2.89s/batch][A
                                                           [AEpochs:   7%|▋         | 14/200 [35:18<7:42:11, 149.10s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:21,  2.88s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.86s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:18,  2.95s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.89s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.88s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.88s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:05,  2.91s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:01,  2.90s/batch][A
Batches:  18%|█▊        | 9/50 [00:26<01:58,  2.88s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.89s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.88s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.86s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:42,  2.84s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:41,  2.90s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:38,  2.89s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.88s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.88s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.89s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.88s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.89s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:20,  2.88s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:18,  2.90s/batch][A
Batches:  48%|████▊     | 24/50 [01:09<01:15,  2.89s/batch][A
Batches:  50%|█████     | 25/50 [01:12<01:11,  2.87s/batch][A
Batches:  52%|█████▏    | 26/50 [01:15<01:09,  2.90s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:06,  2.90s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.87s/batch][A
Batches:  58%|█████▊    | 29/50 [01:23<01:00,  2.87s/batch][A
Batches:  60%|██████    | 30/50 [01:26<00:58,  2.90s/batch][A
Batches:  62%|██████▏   | 31/50 [01:29<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:32<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:35<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:46,  2.88s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:43<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:46<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:49<00:34,  2.88s/batch][A
Batches:  78%|███████▊  | 39/50 [01:52<00:31,  2.89s/batch][A
Batches:  80%|████████  | 40/50 [01:55<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:23,  2.89s/batch][A
Batches:  86%|████████▌ | 43/50 [02:03<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:06<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:09<00:14,  2.90s/batch][A
Batches:  92%|█████████▏| 46/50 [02:12<00:11,  2.88s/batch][A
Batches:  94%|█████████▍| 47/50 [02:15<00:08,  2.89s/batch][A
Batches:  96%|█████████▌| 48/50 [02:18<00:05,  2.88s/batch][A
Batches:  98%|█████████▊| 49/50 [02:21<00:02,  2.91s/batch][A
Batches: 100%|██████████| 50/50 [02:23<00:00,  2.89s/batch][A
                                                           [AEpochs:   8%|▊         | 15/200 [37:45<7:38:10, 148.60s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.85s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.87s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.90s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.87s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.88s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:08,  2.91s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.88s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:01,  2.89s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:58,  2.88s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:56,  2.91s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.89s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.88s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:45,  2.92s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:40,  2.87s/batch][A
Batches:  32%|███▏      | 16/50 [00:46<01:37,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.88s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.86s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.84s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:17,  2.86s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.87s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.88s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:10,  2.92s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:06,  2.89s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:23<01:00,  2.90s/batch][A
Batches:  60%|██████    | 30/50 [01:26<00:57,  2.89s/batch][A
Batches:  62%|██████▏   | 31/50 [01:29<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:32<00:51,  2.88s/batch][A
Batches:  66%|██████▌   | 33/50 [01:35<00:49,  2.91s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:46,  2.90s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:43<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:46<00:37,  2.89s/batch][A
Batches:  76%|███████▌  | 38/50 [01:49<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:52<00:31,  2.87s/batch][A
Batches:  80%|████████  | 40/50 [01:55<00:28,  2.87s/batch][A
Batches:  82%|████████▏ | 41/50 [01:58<00:26,  2.90s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:23,  2.89s/batch][A
Batches:  86%|████████▌ | 43/50 [02:03<00:20,  2.88s/batch][A
Batches:  88%|████████▊ | 44/50 [02:06<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:09<00:14,  2.87s/batch][A
Batches:  92%|█████████▏| 46/50 [02:12<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:15<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:18<00:05,  2.88s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:23<00:00,  2.86s/batch][A
                                                           [AEpochs:   8%|▊         | 16/200 [40:13<7:34:33, 148.23s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.84s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:21,  2.94s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.90s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.86s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.89s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:04,  2.89s/batch][A
Batches:  16%|█▌        | 8/50 [00:23<02:00,  2.87s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:47,  2.89s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.88s/batch][A
Batches:  30%|███       | 15/50 [00:43<01:40,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.85s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.88s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:27,  2.90s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.88s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:20,  2.88s/batch][A
Batches:  46%|████▌     | 23/50 [01:06<01:17,  2.87s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:15,  2.89s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:23<01:00,  2.90s/batch][A
Batches:  60%|██████    | 30/50 [01:26<00:57,  2.89s/batch][A
Batches:  62%|██████▏   | 31/50 [01:29<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.88s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:43,  2.89s/batch][A
Batches:  72%|███████▏  | 36/50 [01:43<00:40,  2.87s/batch][A
Batches:  74%|███████▍  | 37/50 [01:46<00:37,  2.87s/batch][A
Batches:  76%|███████▌  | 38/50 [01:49<00:34,  2.85s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:03<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:06<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:09<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.84s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.89s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.87s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.86s/batch][A
Batches: 100%|██████████| 50/50 [02:23<00:00,  2.85s/batch][A
                                                           [AEpochs:   8%|▊         | 17/200 [42:39<7:30:47, 147.80s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.85s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:12,  2.82s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:09,  2.82s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:10,  2.89s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.86s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.87s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.88s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.89s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.86s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.89s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.84s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:18,  2.89s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:15,  2.89s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:12,  2.89s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.86s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:06,  2.87s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.87s/batch][A
Batches:  58%|█████▊    | 29/50 [01:23<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.83s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.87s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.88s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.87s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:23,  2.90s/batch][A
Batches:  86%|████████▌ | 43/50 [02:03<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.84s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.83s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.82s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.82s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.81s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.84s/batch][A
                                                           [AEpochs:   9%|▉         | 18/200 [45:06<7:27:05, 147.39s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.84s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.88s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.86s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:13,  2.90s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:10,  2.89s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:01,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.86s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.87s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:44,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.84s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:38,  2.82s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:38,  2.88s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.86s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:22,  2.85s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.84s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:06,  2.88s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.87s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.88s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.87s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:46,  2.89s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:43,  2.87s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.87s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.88s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.89s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:26,  2.92s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:23,  2.92s/batch][A
Batches:  86%|████████▌ | 43/50 [02:03<00:20,  2.90s/batch][A
Batches:  88%|████████▊ | 44/50 [02:06<00:17,  2.89s/batch][A
Batches:  90%|█████████ | 45/50 [02:09<00:14,  2.89s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:23<00:00,  2.85s/batch][A
                                                           [AEpochs:  10%|▉         | 19/200 [47:33<7:24:19, 147.29s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.85s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.89s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.89s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.83s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.87s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.88s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.88s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.89s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.86s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.87s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.86s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.85s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.88s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.88s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.88s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:49,  2.89s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:43<00:40,  2.87s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.85s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.83s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.88s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.86s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.88s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.87s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  10%|█         | 20/200 [50:00<7:21:18, 147.11s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:21,  2.89s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:20,  2.93s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.87s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.83s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.88s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.86s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:47,  2.90s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.84s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:35,  2.88s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.88s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.86s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:22,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.83s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:12,  2.88s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.83s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.84s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.87s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.82s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.87s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.88s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.88s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  10%|█         | 21/200 [52:26<7:18:12, 146.88s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:22,  2.91s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.85s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.83s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:09,  2.82s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.83s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.86s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:35,  2.91s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.88s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.88s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.89s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:15,  2.89s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:12,  2.89s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.83s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.83s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.84s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.84s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.84s/batch][A
Batches:  72%|███████▏  | 36/50 [01:43<00:40,  2.92s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.90s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.88s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.89s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.84s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.83s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.87s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.83s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  11%|█         | 22/200 [54:52<7:15:20, 146.74s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:18,  2.83s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.84s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.86s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.88s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.89s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.83s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.84s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.86s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.87s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.88s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.88s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.87s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.87s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.90s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.87s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.86s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.83s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  12%|█▏        | 23/200 [57:19<7:12:53, 146.74s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:20,  2.86s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.88s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.85s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:13,  2.89s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:02,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.88s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.88s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.88s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.88s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:06,  2.89s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.83s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.87s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:43,  2.89s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.85s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.83s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.85s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.83s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.83s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.82s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.83s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.83s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.88s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.87s/batch][A
                                                           [AEpochs:  12%|█▏        | 24/200 [59:45<7:10:02, 146.61s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:16,  2.78s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:13,  2.78s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.86s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.84s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:03,  2.87s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.86s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.88s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.86s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.86s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:43,  2.86s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.88s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.83s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.83s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.88s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:18,  2.82s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.84s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.86s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.83s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.82s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.89s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.90s/batch][A
                                                           [AEpochs:  12%|█▎        | 25/200 [1:02:12<7:07:13, 146.47s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:16,  2.79s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:13,  2.79s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:11,  2.80s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.84s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:06,  2.81s/batch][A
Batches:  12%|█▏        | 6/50 [00:16<02:04,  2.82s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:01,  2.89s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.87s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.84s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:45,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.84s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.83s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.84s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.83s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.87s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:08,  2.83s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:06,  2.88s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.84s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.82s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:43,  2.87s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:33,  2.83s/batch][A
Batches:  78%|███████▊  | 39/50 [01:50<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.87s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.88s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.87s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.83s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.84s/batch][A
                                                           [AEpochs:  13%|█▎        | 26/200 [1:04:37<7:04:11, 146.27s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:23,  2.92s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:19,  2.90s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.89s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.87s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.88s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.83s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.83s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.86s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.88s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.85s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:27,  2.84s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.83s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.87s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.83s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.83s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.81s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.83s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.84s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:43,  2.88s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.88s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.85s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.81s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.83s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.82s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.83s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.82s/batch][A
                                                           [AEpochs:  14%|█▎        | 27/200 [1:07:03<7:01:26, 146.17s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:22,  2.90s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.86s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.83s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:08,  2.80s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:58,  2.88s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:45,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.84s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:35,  2.88s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.85s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:27,  2.83s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:24,  2.82s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.82s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:10,  2.83s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.83s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.86s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.83s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.83s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:33,  2.83s/batch][A
Batches:  78%|███████▊  | 39/50 [01:50<00:30,  2.82s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.84s/batch][A
Batches:  90%|█████████ | 45/50 [02:07<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.83s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.87s/batch][A
                                                           [AEpochs:  14%|█▍        | 28/200 [1:09:29<6:58:53, 146.12s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:23,  2.93s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.88s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.87s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.87s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.88s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:01,  2.88s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:44,  2.83s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:41,  2.82s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:38,  2.82s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:35,  2.81s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.86s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.83s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:10,  2.84s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.83s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.82s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.83s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.88s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.87s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.88s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.87s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.88s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.87s/batch][A
                                                           [AEpochs:  14%|█▍        | 29/200 [1:11:56<6:56:36, 146.18s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.84s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.82s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:12,  2.81s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:08,  2.79s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:16<02:05,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:41,  2.83s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.84s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.88s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.85s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.84s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:26,  2.87s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.87s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:15,  2.89s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.87s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.85s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.83s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.87s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.84s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.87s/batch][A
                                                           [AEpochs:  15%|█▌        | 30/200 [1:14:22<6:54:15, 146.21s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:18,  2.82s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:14,  2.81s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:11,  2.79s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.83s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:16<02:04,  2.82s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.88s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.88s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.86s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.88s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.84s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.83s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.83s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.87s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.87s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.83s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.84s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.82s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.83s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.82s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:23,  2.90s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.89s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.87s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.88s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.83s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  16%|█▌        | 31/200 [1:16:48<6:52:01, 146.28s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:17,  2.80s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.88s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.83s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.82s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.87s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.86s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:43,  2.88s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.88s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.86s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.83s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:07,  2.83s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:04,  2.82s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:01,  2.81s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.83s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:50,  2.83s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.88s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.87s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.82s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.86s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.84s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.83s/batch][A
                                                           [AEpochs:  16%|█▌        | 32/200 [1:19:15<6:49:33, 146.27s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:20,  2.86s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:20,  2.92s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.90s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.88s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:02,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.84s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:35,  2.88s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.84s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.87s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:22,  2.85s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.88s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.87s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.87s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:43,  2.88s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.87s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.83s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.83s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.84s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.84s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.83s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.84s/batch][A
                                                           [AEpochs:  16%|█▋        | 33/200 [1:21:41<6:47:07, 146.27s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.85s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.81s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:12,  2.81s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.88s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.84s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.82s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.84s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:44,  2.83s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:41,  2.81s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:38,  2.81s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.83s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.83s/batch][A
Batches:  38%|███▊      | 19/50 [00:53<01:27,  2.82s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.84s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.85s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.85s/batch][A
Batches:  50%|█████     | 25/50 [01:10<01:10,  2.84s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:07,  2.82s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.86s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.85s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:30<00:51,  2.84s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.83s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:44<00:36,  2.83s/batch][A
Batches:  76%|███████▌  | 38/50 [01:47<00:33,  2.82s/batch][A
Batches:  78%|███████▊  | 39/50 [01:50<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.83s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.86s/batch][A
Batches:  88%|████████▊ | 44/50 [02:04<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:07<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.89s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.83s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.82s/batch][A
Batches: 100%|██████████| 50/50 [02:21<00:00,  2.83s/batch][A
                                                           [AEpochs:  17%|█▋        | 34/200 [1:24:07<6:44:11, 146.09s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:15,  2.76s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.81s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:12,  2.82s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:44,  2.83s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:41,  2.81s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.84s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.83s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.83s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.88s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.83s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:10<01:10,  2.82s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.83s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.83s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:30<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.89s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:50<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.83s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:16,  2.83s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.84s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.87s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.86s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.84s/batch][A
                                                           [AEpochs:  18%|█▊        | 35/200 [1:26:33<6:42:19, 146.30s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:08<07:02,  8.62s/batch][A
Batches:   4%|▍         | 2/50 [00:11<04:09,  5.21s/batch][A
Batches:   6%|▌         | 3/50 [00:24<06:53,  8.80s/batch][A
Batches:   8%|▊         | 4/50 [00:27<04:55,  6.42s/batch][A
Batches:  10%|█         | 5/50 [00:30<03:50,  5.11s/batch][A
Batches:  12%|█▏        | 6/50 [00:32<03:11,  4.36s/batch][A
Batches:  14%|█▍        | 7/50 [00:35<02:46,  3.88s/batch][A
Batches:  16%|█▌        | 8/50 [00:38<02:29,  3.56s/batch][A
Batches:  18%|█▊        | 9/50 [00:41<02:16,  3.34s/batch][A
Batches:  20%|██        | 10/50 [00:44<02:09,  3.24s/batch][A
Batches:  22%|██▏       | 11/50 [00:47<02:01,  3.12s/batch][A
Batches:  24%|██▍       | 12/50 [00:50<01:55,  3.04s/batch][A
Batches:  26%|██▌       | 13/50 [00:53<01:50,  2.99s/batch][A
Batches:  28%|██▊       | 14/50 [00:55<01:45,  2.93s/batch][A
Batches:  30%|███       | 15/50 [00:58<01:41,  2.90s/batch][A
Batches:  32%|███▏      | 16/50 [01:01<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [01:04<01:34,  2.87s/batch][A
Batches:  36%|███▌      | 18/50 [01:07<01:31,  2.85s/batch][A
Batches:  38%|███▊      | 19/50 [01:10<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [01:12<01:25,  2.84s/batch][A
Batches:  42%|████▏     | 21/50 [01:15<01:23,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:18<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:21<01:16,  2.83s/batch][A
Batches:  48%|████▊     | 24/50 [01:24<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:27<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:30<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:32<01:05,  2.83s/batch][A
Batches:  56%|█████▌    | 28/50 [01:35<01:02,  2.83s/batch][A
Batches:  58%|█████▊    | 29/50 [01:38<01:00,  2.86s/batch][A
Batches:  60%|██████    | 30/50 [01:41<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:44<00:54,  2.88s/batch][A
Batches:  64%|██████▍   | 32/50 [01:47<00:52,  2.89s/batch][A
Batches:  66%|██████▌   | 33/50 [01:50<00:48,  2.87s/batch][A
Batches:  68%|██████▊   | 34/50 [01:52<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:55<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:58<00:40,  2.88s/batch][A
Batches:  74%|███████▍  | 37/50 [02:01<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [02:04<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [02:07<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [02:10<00:28,  2.88s/batch][A
Batches:  82%|████████▏ | 41/50 [02:12<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [02:15<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:18<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:21<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:24<00:14,  2.88s/batch][A
Batches:  92%|█████████▏| 46/50 [02:27<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:30<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:33<00:05,  2.88s/batch][A
Batches:  98%|█████████▊| 49/50 [02:35<00:02,  2.88s/batch][A
Batches: 100%|██████████| 50/50 [02:38<00:00,  2.86s/batch][A
                                                           [AEpochs:  18%|█▊        | 36/200 [1:29:16<6:53:17, 151.20s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.86s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.89s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.88s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.86s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.83s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.84s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:44,  2.82s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:38,  2.82s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:35,  2.81s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:53<01:28,  2.84s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:07,  2.82s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:04,  2.82s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.82s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:30<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.87s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.85s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.83s/batch][A
Batches:  78%|███████▊  | 39/50 [01:50<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.87s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.84s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.87s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.87s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  18%|█▊        | 37/200 [1:31:42<6:46:23, 149.59s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:21,  2.89s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.85s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.85s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.83s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.88s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.82s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.86s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:35,  2.89s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.89s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.87s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.84s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:50,  2.82s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.84s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.83s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.83s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:26,  2.89s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.84s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:16,  2.82s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.81s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.81s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.82s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.86s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.84s/batch][A
                                                           [AEpochs:  19%|█▉        | 38/200 [1:34:08<6:40:54, 148.49s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:18,  2.82s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.86s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.83s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:00,  2.81s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.83s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.90s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.89s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.88s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:44,  2.89s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.83s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:24,  2.83s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.83s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.88s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.86s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.85s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.87s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:09,  2.88s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.86s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.86s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.84s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.83s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.82s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.89s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.87s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.87s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.87s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.86s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.83s/batch][A
                                                           [AEpochs:  20%|█▉        | 39/200 [1:36:34<6:36:27, 147.75s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:20,  2.86s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:20,  2.92s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.87s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.84s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.86s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.86s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.87s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.84s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.86s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.85s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.83s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.82s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.82s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.82s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.86s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.84s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:50,  2.83s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.87s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.87s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.88s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.84s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.87s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.86s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.87s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  20%|██        | 40/200 [1:39:01<6:33:31, 147.57s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:16,  2.78s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.83s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:12,  2.81s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:09,  2.81s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.86s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.83s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.86s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.86s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.83s/batch][A
Batches:  38%|███▊      | 19/50 [00:53<01:27,  2.83s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.88s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.83s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.83s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.84s/batch][A
Batches:  64%|██████▍   | 32/50 [01:30<00:50,  2.82s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.83s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.84s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.88s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.85s/batch][A
Batches:  78%|███████▊  | 39/50 [01:50<00:31,  2.83s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.84s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:07<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.84s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.83s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.83s/batch][A
                                                           [AEpochs:  20%|██        | 41/200 [1:41:27<6:29:35, 147.01s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:16,  2.80s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.85s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.90s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.88s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.84s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.87s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.86s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.86s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.88s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.86s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.86s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.83s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.86s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.85s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.87s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:09,  2.91s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:06,  2.89s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.87s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.84s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:50,  2.83s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.83s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.82s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.82s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.85s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.87s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.82s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.87s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.83s/batch][A
                                                           [AEpochs:  21%|██        | 42/200 [1:43:53<6:26:34, 146.80s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:16,  2.78s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.88s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.86s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.88s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.84s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.89s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:41,  2.89s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.84s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.85s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.87s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.86s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.88s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:50,  2.83s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.84s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.83s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.86s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.87s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  22%|██▏       | 43/200 [1:46:19<6:23:41, 146.63s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:17,  2.81s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.89s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:03,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.84s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.83s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:43,  2.88s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:41,  2.89s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.84s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.86s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.84s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.83s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:10,  2.84s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.86s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.84s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.87s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.85s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.85s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.87s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.84s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.86s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.84s/batch][A
                                                           [AEpochs:  22%|██▏       | 44/200 [1:48:45<6:20:51, 146.49s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:18,  2.82s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.82s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.85s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.83s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:06,  2.81s/batch][A
Batches:  12%|█▏        | 6/50 [00:16<02:04,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:57,  2.80s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:55,  2.82s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:33<01:47,  2.84s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:44,  2.82s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.87s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.86s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.88s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.87s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.86s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.85s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.87s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.88s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.85s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.83s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.87s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.84s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:16,  2.83s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  22%|██▎       | 45/200 [1:51:12<6:18:14, 146.42s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.85s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:19,  2.91s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.84s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.86s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.84s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.87s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:27,  2.84s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.83s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.89s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.86s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.87s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.88s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.87s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:46,  2.88s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.82s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.83s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.79s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  23%|██▎       | 46/200 [1:53:38<6:15:45, 146.40s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:21,  2.88s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.88s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.84s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.84s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.87s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.82s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.84s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.88s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.86s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.87s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.87s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.86s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.87s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.88s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.83s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.85s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.83s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.88s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.87s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.85s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.87s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.87s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.87s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.86s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.87s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  24%|██▎       | 47/200 [1:56:05<6:13:28, 146.46s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.85s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.89s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.89s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.88s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:07,  2.89s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:02,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.82s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:44,  2.83s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.86s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.89s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.89s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.89s/batch][A
Batches:  44%|████▍     | 22/50 [01:03<01:20,  2.88s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.87s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.85s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.86s/batch][A
Batches:  54%|█████▍    | 27/50 [01:21<01:35,  4.17s/batch][A
Batches:  56%|█████▌    | 28/50 [01:24<01:23,  3.81s/batch][A
Batches:  58%|█████▊    | 29/50 [01:29<01:25,  4.05s/batch][A
Batches:  60%|██████    | 30/50 [01:32<01:13,  3.69s/batch][A
Batches:  62%|██████▏   | 31/50 [01:34<01:05,  3.43s/batch][A
Batches:  64%|██████▍   | 32/50 [01:37<00:58,  3.25s/batch][A
Batches:  66%|██████▌   | 33/50 [01:40<00:53,  3.16s/batch][A
Batches:  68%|██████▊   | 34/50 [01:43<00:48,  3.05s/batch][A
Batches:  70%|███████   | 35/50 [01:46<00:44,  2.99s/batch][A
Batches:  72%|███████▏  | 36/50 [01:49<00:41,  2.95s/batch][A
Batches:  74%|███████▍  | 37/50 [01:52<00:38,  2.94s/batch][A
Batches:  76%|███████▌  | 38/50 [01:58<00:46,  3.89s/batch][A
Batches:  78%|███████▊  | 39/50 [02:00<00:39,  3.57s/batch][A
Batches:  80%|████████  | 40/50 [02:04<00:34,  3.49s/batch][A
Batches:  82%|████████▏ | 41/50 [02:07<00:29,  3.30s/batch][A
Batches:  84%|████████▍ | 42/50 [02:09<00:25,  3.16s/batch][A
Batches:  86%|████████▌ | 43/50 [02:12<00:21,  3.07s/batch][A
Batches:  88%|████████▊ | 44/50 [02:15<00:18,  3.04s/batch][A
Batches:  90%|█████████ | 45/50 [02:18<00:14,  2.97s/batch][A
Batches:  92%|█████████▏| 46/50 [02:21<00:11,  2.94s/batch][A
Batches:  94%|█████████▍| 47/50 [02:24<00:08,  2.90s/batch][A
Batches:  96%|█████████▌| 48/50 [02:27<00:05,  2.90s/batch][A
Batches:  98%|█████████▊| 49/50 [02:30<00:02,  2.88s/batch][A
Batches: 100%|██████████| 50/50 [02:32<00:00,  2.86s/batch][A
                                                           [AEpochs:  24%|██▍       | 48/200 [1:58:42<6:19:08, 149.66s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:15,  2.77s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.85s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:12,  2.82s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:09,  2.82s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.86s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.83s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.89s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:43,  2.88s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.87s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.87s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.84s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.83s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.82s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.83s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.82s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.83s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.87s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.82s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.82s/batch][A
                                                           [AEpochs:  24%|██▍       | 49/200 [2:01:08<6:13:48, 148.53s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:22,  2.91s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.87s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.85s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:09,  2.82s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.82s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.82s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:52,  2.82s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.83s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.88s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.88s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.84s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:27,  2.83s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.87s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.85s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.86s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.86s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:03,  2.89s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.88s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.82s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.82s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:33,  2.82s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.87s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.87s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.83s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.82s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.83s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.81s/batch][A
                                                           [AEpochs:  25%|██▌       | 50/200 [2:03:34<6:09:24, 147.76s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:17,  2.81s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.88s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.84s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:10,  2.90s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:07,  2.90s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:04,  2.89s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.86s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.83s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:41,  2.82s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:38,  2.82s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:35,  2.82s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:03,  2.87s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.86s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.82s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.82s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.84s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.83s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.82s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.80s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.83s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.84s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.83s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.84s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.84s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  26%|██▌       | 51/200 [2:06:00<6:05:39, 147.24s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:22,  2.91s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.83s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.88s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.86s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:41,  2.89s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:35,  2.88s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.88s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.87s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:22,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.87s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:06,  2.88s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:23<00:59,  2.86s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.89s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:52,  2.89s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.88s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:43<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:33,  2.83s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:03<00:20,  2.88s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.88s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.83s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.82s/batch][A
                                                           [AEpochs:  26%|██▌       | 52/200 [2:08:26<6:02:38, 147.02s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:20,  2.87s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.86s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.84s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.89s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.87s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:01,  2.89s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:41,  2.83s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:38,  2.81s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.83s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.83s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.82s/batch][A
Batches:  38%|███▊      | 19/50 [00:53<01:27,  2.81s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.83s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.82s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.85s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.83s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:52,  2.90s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.87s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.89s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.88s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.83s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.83s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.86s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.84s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.83s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.83s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.82s/batch][A
                                                           [AEpochs:  26%|██▋       | 53/200 [2:10:52<5:59:22, 146.68s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:18,  2.83s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.86s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.85s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.84s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.89s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:52,  2.88s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:47,  2.89s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.88s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.85s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.84s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.88s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.87s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.86s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.83s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.87s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.84s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.89s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.87s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.87s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.87s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.87s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  27%|██▋       | 54/200 [2:13:19<5:56:48, 146.63s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:22,  2.90s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.85s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.85s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:09,  2.82s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:00,  2.87s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.83s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.88s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:43,  2.86s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.87s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.87s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.83s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.82s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.87s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.85s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:53,  2.83s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:33,  2.82s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.83s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.87s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.88s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  28%|██▊       | 55/200 [2:15:45<5:54:01, 146.49s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:25,  2.97s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:18,  2.88s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.85s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.88s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.86s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.82s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.86s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.86s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.84s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.87s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.84s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.89s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.88s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.88s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.87s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.83s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:50,  2.83s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.83s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.82s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.82s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.83s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.86s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.83s/batch][A
                                                           [AEpochs:  28%|██▊       | 56/200 [2:18:11<5:51:14, 146.35s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:21,  2.89s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.83s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.84s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.83s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:09,  2.87s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.88s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:01,  2.90s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:58,  2.89s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:55,  2.88s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.86s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:40<01:42,  2.83s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:38,  2.82s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.83s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.84s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.88s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.85s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.87s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.85s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.84s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.82s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.85s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:23,  2.88s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.89s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.87s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.88s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.83s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.84s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  28%|██▊       | 57/200 [2:20:37<5:48:39, 146.29s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.84s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.82s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:11,  2.81s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.86s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:16<02:04,  2.83s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.86s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:45,  2.86s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.86s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:30,  2.84s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.88s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.89s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.88s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.86s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.86s/batch][A
Batches:  56%|█████▌    | 28/50 [01:20<01:03,  2.89s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.89s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.88s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.84s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:43,  2.87s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.84s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.87s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.86s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.84s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.83s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.83s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.84s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.84s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.87s/batch][A
                                                           [AEpochs:  29%|██▉       | 58/200 [2:23:03<5:46:20, 146.34s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:17,  2.80s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:13,  2.77s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:12,  2.82s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.84s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:16<02:04,  2.83s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<02:01,  2.89s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:57,  2.87s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.87s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.88s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.84s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.88s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.86s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.88s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:30,  2.91s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.89s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:23,  2.88s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.87s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.83s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.83s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.83s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.87s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.84s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.86s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.84s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.82s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.84s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.87s/batch][A
                                                           [AEpochs:  30%|██▉       | 59/200 [2:25:30<5:43:56, 146.36s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:19,  2.84s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:16,  2.84s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:13,  2.84s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:12,  2.88s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.84s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.83s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:44,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:41,  2.83s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.88s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.87s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.87s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:06,  2.88s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:03,  2.87s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.86s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.89s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.88s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.88s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:46,  2.92s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:43,  2.89s/batch][A
Batches:  72%|███████▏  | 36/50 [01:43<00:40,  2.88s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [02:00<00:22,  2.87s/batch][A
Batches:  86%|████████▌ | 43/50 [02:03<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.88s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.88s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:20<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:23<00:00,  2.87s/batch][A
                                                           [AEpochs:  30%|███       | 60/200 [2:27:56<5:41:34, 146.39s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:17,  2.80s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:14,  2.81s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.86s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.84s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.86s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.85s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.84s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.85s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.83s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.84s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:45,  2.84s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.84s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:41,  2.89s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.83s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:10,  2.84s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.82s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.84s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.83s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.85s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.85s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.84s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.84s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.83s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.82s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.82s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:16,  2.82s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.84s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.86s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.86s/batch][A
                                                           [AEpochs:  30%|███       | 61/200 [2:30:22<5:38:47, 146.24s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:22,  2.91s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.86s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:16,  2.90s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.87s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.84s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.86s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:02,  2.85s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.83s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.82s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:44,  2.83s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.86s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.87s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.87s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.84s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.82s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.82s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.84s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.83s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.83s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.87s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.88s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.87s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.88s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.88s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.87s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.87s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.87s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:17<00:05,  2.87s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  31%|███       | 62/200 [2:32:48<5:36:22, 146.25s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:17,  2.80s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:17,  2.87s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.86s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.85s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:05,  2.86s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:01,  2.83s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:58,  2.83s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:51,  2.86s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:43,  2.87s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:35,  2.82s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:34,  2.88s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.88s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.87s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:23,  2.88s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.85s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.86s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.86s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.88s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.88s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.87s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:49,  2.89s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:46,  2.88s/batch][A
Batches:  70%|███████   | 35/50 [01:40<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.85s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.87s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.83s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.83s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:16,  2.83s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.84s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.83s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.83s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.89s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  32%|███▏      | 63/200 [2:35:15<5:33:54, 146.24s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:20,  2.86s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.82s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.87s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:10,  2.84s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.83s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.88s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.87s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.84s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.86s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.85s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:44,  2.83s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.85s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.85s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.86s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.85s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.83s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:10,  2.83s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:56,  2.84s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.84s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:50,  2.83s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.87s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.84s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.88s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.83s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:16,  2.83s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:10<00:11,  2.84s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.89s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.89s/batch][A
                                                           [AEpochs:  32%|███▏      | 64/200 [2:37:41<5:31:21, 146.19s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:15,  2.77s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.82s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:14,  2.86s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:09,  2.83s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:06,  2.81s/batch][A
Batches:  12%|█▏        | 6/50 [00:16<02:04,  2.82s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:53,  2.84s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.83s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:49,  2.87s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.84s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.87s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.83s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.84s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:29,  2.90s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:26,  2.88s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.86s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.86s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.84s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:10,  2.83s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:07,  2.82s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.86s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.84s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.85s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.87s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.86s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.86s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:53<00:28,  2.84s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.82s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.85s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.84s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:16,  2.83s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.82s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.87s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.88s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.88s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.86s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.87s/batch][A
                                                           [AEpochs:  32%|███▎      | 65/200 [2:40:07<5:28:49, 146.15s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:16,  2.79s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:15,  2.83s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:11,  2.81s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.87s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:08,  2.85s/batch][A
Batches:  12%|█▏        | 6/50 [00:16<02:04,  2.83s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:52,  2.82s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:33<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:36<01:43,  2.81s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:40,  2.80s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:39,  2.84s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:36,  2.84s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.84s/batch][A
Batches:  36%|███▌      | 18/50 [00:50<01:30,  2.83s/batch][A
Batches:  38%|███▊      | 19/50 [00:53<01:29,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:25,  2.85s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.85s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:17,  2.88s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:14,  2.86s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:13<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:03,  2.88s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.88s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.86s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.88s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.86s/batch][A
Batches:  66%|██████▌   | 33/50 [01:33<00:48,  2.85s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:46,  2.88s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:43,  2.87s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:36,  2.85s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.87s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.85s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.87s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:20,  2.87s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.85s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.86s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.86s/batch][A
Batches:  94%|█████████▍| 47/50 [02:13<00:08,  2.85s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.85s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.91s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.90s/batch][A
                                                           [AEpochs:  33%|███▎      | 66/200 [2:42:33<5:26:29, 146.19s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:18,  2.83s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:14,  2.81s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.89s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.87s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.84s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:04,  2.83s/batch][A
Batches:  14%|█▍        | 7/50 [00:19<02:02,  2.86s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.85s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:56,  2.84s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.87s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:48,  2.85s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:45,  2.85s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.84s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.88s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.87s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:35,  2.88s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:32,  2.90s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:28,  2.87s/batch][A
Batches:  40%|████      | 20/50 [00:57<01:25,  2.86s/batch][A
Batches:  42%|████▏     | 21/50 [01:00<01:22,  2.86s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:20,  2.87s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.82s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:10,  2.82s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.87s/batch][A
Batches:  54%|█████▍    | 27/50 [01:17<01:05,  2.84s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.85s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<00:59,  2.84s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.85s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.83s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.86s/batch][A
Batches:  68%|██████▊   | 34/50 [01:37<00:46,  2.88s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:43,  2.87s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:40,  2.87s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.87s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.85s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.85s/batch][A
Batches:  82%|████████▏ | 41/50 [01:57<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.82s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.83s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:16,  2.82s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.83s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.82s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.85s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.84s/batch][A
                                                           [AEpochs:  34%|███▎      | 67/200 [2:44:59<5:23:56, 146.14s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:02<02:17,  2.81s/batch][A
Batches:   4%|▍         | 2/50 [00:05<02:19,  2.91s/batch][A
Batches:   6%|▌         | 3/50 [00:08<02:15,  2.88s/batch][A
Batches:   8%|▊         | 4/50 [00:11<02:11,  2.86s/batch][A
Batches:  10%|█         | 5/50 [00:14<02:07,  2.84s/batch][A
Batches:  12%|█▏        | 6/50 [00:17<02:06,  2.87s/batch][A
Batches:  14%|█▍        | 7/50 [00:20<02:03,  2.87s/batch][A
Batches:  16%|█▌        | 8/50 [00:22<01:59,  2.84s/batch][A
Batches:  18%|█▊        | 9/50 [00:25<01:55,  2.83s/batch][A
Batches:  20%|██        | 10/50 [00:28<01:54,  2.85s/batch][A
Batches:  22%|██▏       | 11/50 [00:31<01:50,  2.84s/batch][A
Batches:  24%|██▍       | 12/50 [00:34<01:47,  2.83s/batch][A
Batches:  26%|██▌       | 13/50 [00:37<01:46,  2.87s/batch][A
Batches:  28%|██▊       | 14/50 [00:39<01:42,  2.85s/batch][A
Batches:  30%|███       | 15/50 [00:42<01:40,  2.86s/batch][A
Batches:  32%|███▏      | 16/50 [00:45<01:37,  2.86s/batch][A
Batches:  34%|███▍      | 17/50 [00:48<01:33,  2.85s/batch][A
Batches:  36%|███▌      | 18/50 [00:51<01:31,  2.87s/batch][A
Batches:  38%|███▊      | 19/50 [00:54<01:27,  2.83s/batch][A
Batches:  40%|████      | 20/50 [00:56<01:24,  2.82s/batch][A
Batches:  42%|████▏     | 21/50 [00:59<01:22,  2.84s/batch][A
Batches:  44%|████▍     | 22/50 [01:02<01:19,  2.85s/batch][A
Batches:  46%|████▌     | 23/50 [01:05<01:16,  2.84s/batch][A
Batches:  48%|████▊     | 24/50 [01:08<01:13,  2.83s/batch][A
Batches:  50%|█████     | 25/50 [01:11<01:11,  2.85s/batch][A
Batches:  52%|█████▏    | 26/50 [01:14<01:08,  2.85s/batch][A
Batches:  54%|█████▍    | 27/50 [01:16<01:05,  2.85s/batch][A
Batches:  56%|█████▌    | 28/50 [01:19<01:02,  2.83s/batch][A
Batches:  58%|█████▊    | 29/50 [01:22<01:00,  2.89s/batch][A
Batches:  60%|██████    | 30/50 [01:25<00:57,  2.87s/batch][A
Batches:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/batch][A
Batches:  64%|██████▍   | 32/50 [01:31<00:51,  2.85s/batch][A
Batches:  66%|██████▌   | 33/50 [01:34<00:48,  2.87s/batch][A
Batches:  68%|██████▊   | 34/50 [01:36<00:45,  2.86s/batch][A
Batches:  70%|███████   | 35/50 [01:39<00:42,  2.85s/batch][A
Batches:  72%|███████▏  | 36/50 [01:42<00:39,  2.84s/batch][A
Batches:  74%|███████▍  | 37/50 [01:45<00:37,  2.86s/batch][A
Batches:  76%|███████▌  | 38/50 [01:48<00:34,  2.86s/batch][A
Batches:  78%|███████▊  | 39/50 [01:51<00:31,  2.86s/batch][A
Batches:  80%|████████  | 40/50 [01:54<00:28,  2.86s/batch][A
Batches:  82%|████████▏ | 41/50 [01:56<00:25,  2.86s/batch][A
Batches:  84%|████████▍ | 42/50 [01:59<00:22,  2.84s/batch][A
Batches:  86%|████████▌ | 43/50 [02:02<00:19,  2.85s/batch][A
Batches:  88%|████████▊ | 44/50 [02:05<00:17,  2.88s/batch][A
Batches:  90%|█████████ | 45/50 [02:08<00:14,  2.87s/batch][A
Batches:  92%|█████████▏| 46/50 [02:11<00:11,  2.85s/batch][A
Batches:  94%|█████████▍| 47/50 [02:14<00:08,  2.84s/batch][A
Batches:  96%|█████████▌| 48/50 [02:16<00:05,  2.86s/batch][A
Batches:  98%|█████████▊| 49/50 [02:19<00:02,  2.86s/batch][A
Batches: 100%|██████████| 50/50 [02:22<00:00,  2.85s/batch][A
                                                           [AEpochs:  34%|███▎      | 67/200 [2:47:22<5:32:14, 149.88s/epoch]
