/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/albumentations/check_version.py:51: UserWarning: Error fetching version info <urlopen error [Errno 101] Network is unreachable>
  data = fetch_version_info()
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epochs:   0%|          | 0/200 [00:00<?, ?epoch/s]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:29<24:29, 29.99s/batch][A
Batches:   4%|▍         | 2/50 [00:52<20:19, 25.41s/batch][A
Batches:   6%|▌         | 3/50 [01:14<18:44, 23.93s/batch][A
Batches:   8%|▊         | 4/50 [01:36<17:52, 23.31s/batch][A
Batches:  10%|█         | 5/50 [01:58<17:10, 22.91s/batch][A
Batches:  12%|█▏        | 6/50 [02:21<16:37, 22.66s/batch][A
Batches:  14%|█▍        | 7/50 [02:43<16:07, 22.50s/batch][A
Batches:  16%|█▌        | 8/50 [03:05<15:38, 22.35s/batch][A
Batches:  18%|█▊        | 9/50 [03:27<15:14, 22.30s/batch][A
Batches:  20%|██        | 10/50 [03:49<14:49, 22.25s/batch][A
Batches:  22%|██▏       | 11/50 [04:11<14:26, 22.23s/batch][A
Batches:  24%|██▍       | 12/50 [04:34<14:05, 22.24s/batch][A
Batches:  26%|██▌       | 13/50 [04:56<13:43, 22.24s/batch][A
Batches:  28%|██▊       | 14/50 [05:18<13:19, 22.20s/batch][A
Batches:  30%|███       | 15/50 [05:40<12:54, 22.14s/batch][A
Batches:  32%|███▏      | 16/50 [06:02<12:33, 22.16s/batch][A
Batches:  34%|███▍      | 17/50 [06:24<12:09, 22.12s/batch][A
Batches:  36%|███▌      | 18/50 [06:46<11:48, 22.13s/batch][A
Batches:  38%|███▊      | 19/50 [07:08<11:25, 22.11s/batch][A
Batches:  40%|████      | 20/50 [07:31<11:05, 22.18s/batch][A
Batches:  42%|████▏     | 21/50 [07:53<10:42, 22.15s/batch][A
Batches:  44%|████▍     | 22/50 [08:15<10:19, 22.14s/batch][A
Batches:  46%|████▌     | 23/50 [08:37<09:57, 22.13s/batch][A
Batches:  48%|████▊     | 24/50 [08:59<09:35, 22.12s/batch][A
Batches:  50%|█████     | 25/50 [09:21<09:13, 22.14s/batch][A
Batches:  52%|█████▏    | 26/50 [09:43<08:51, 22.16s/batch][A
Batches:  54%|█████▍    | 27/50 [10:06<08:29, 22.17s/batch][A
Batches:  56%|█████▌    | 28/50 [10:28<08:08, 22.22s/batch][A
Batches:  58%|█████▊    | 29/50 [10:50<07:45, 22.16s/batch][A
Batches:  60%|██████    | 30/50 [11:12<07:23, 22.15s/batch][A
Batches:  62%|██████▏   | 31/50 [11:34<07:00, 22.14s/batch][A
Batches:  64%|██████▍   | 32/50 [11:57<06:39, 22.17s/batch][A
Batches:  66%|██████▌   | 33/50 [12:19<06:16, 22.16s/batch][A
Batches:  68%|██████▊   | 34/50 [12:41<05:54, 22.18s/batch][A
Batches:  70%|███████   | 35/50 [13:03<05:33, 22.20s/batch][A
Batches:  72%|███████▏  | 36/50 [13:25<05:10, 22.18s/batch][A
Batches:  74%|███████▍  | 37/50 [13:47<04:48, 22.18s/batch][A
Batches:  76%|███████▌  | 38/50 [14:10<04:25, 22.15s/batch][A
Batches:  78%|███████▊  | 39/50 [14:32<04:04, 22.19s/batch][A
Batches:  80%|████████  | 40/50 [14:54<03:41, 22.14s/batch][A
Batches:  82%|████████▏ | 41/50 [15:16<03:19, 22.15s/batch][A
Batches:  84%|████████▍ | 42/50 [15:38<02:57, 22.18s/batch][A
Batches:  86%|████████▌ | 43/50 [16:00<02:35, 22.15s/batch][A
Batches:  88%|████████▊ | 44/50 [16:22<02:12, 22.13s/batch][A
Batches:  90%|█████████ | 45/50 [16:45<01:50, 22.12s/batch][A
Batches:  92%|█████████▏| 46/50 [17:07<01:28, 22.19s/batch][A
Batches:  94%|█████████▍| 47/50 [17:29<01:06, 22.13s/batch][A
Batches:  96%|█████████▌| 48/50 [17:51<00:44, 22.07s/batch][A
Batches:  98%|█████████▊| 49/50 [18:13<00:22, 22.15s/batch][A
Batches: 100%|██████████| 50/50 [18:35<00:00, 22.13s/batch][A
                                                           [AEpochs:   0%|          | 1/200 [18:39<61:54:05, 1119.83s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:07, 22.19s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:45, 22.19s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:24, 22.22s/batch][A
Batches:   8%|▊         | 4/50 [01:28<17:01, 22.20s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:37, 22.18s/batch][A
Batches:  12%|█▏        | 6/50 [02:13<16:16, 22.20s/batch][A
Batches:  14%|█▍        | 7/50 [02:35<15:54, 22.20s/batch][A
Batches:  16%|█▌        | 8/50 [02:57<15:32, 22.21s/batch][A
Batches:  18%|█▊        | 9/50 [03:19<15:10, 22.20s/batch][A
Batches:  20%|██        | 10/50 [03:41<14:46, 22.17s/batch][A
Batches:  22%|██▏       | 11/50 [04:04<14:25, 22.18s/batch][A
Batches:  24%|██▍       | 12/50 [04:26<14:04, 22.23s/batch][A
Batches:  26%|██▌       | 13/50 [04:48<13:43, 22.26s/batch][A
Batches:  28%|██▊       | 14/50 [05:11<13:23, 22.32s/batch][A
Batches:  30%|███       | 15/50 [05:33<12:58, 22.24s/batch][A
Batches:  32%|███▏      | 16/50 [05:55<12:36, 22.24s/batch][A
Batches:  34%|███▍      | 17/50 [06:17<12:12, 22.20s/batch][A
Batches:  36%|███▌      | 18/50 [06:39<11:49, 22.18s/batch][A
Batches:  38%|███▊      | 19/50 [07:01<11:27, 22.19s/batch][A
Batches:  40%|████      | 20/50 [07:24<11:05, 22.20s/batch][A
Batches:  42%|████▏     | 21/50 [07:46<10:43, 22.18s/batch][A
Batches:  44%|████▍     | 22/50 [08:08<10:19, 22.12s/batch][A
Batches:  46%|████▌     | 23/50 [08:30<09:57, 22.12s/batch][A
Batches:  48%|████▊     | 24/50 [08:52<09:34, 22.09s/batch][A
Batches:  50%|█████     | 25/50 [09:14<09:12, 22.11s/batch][A
Batches:  52%|█████▏    | 26/50 [09:36<08:51, 22.14s/batch][A
Batches:  54%|█████▍    | 27/50 [09:59<08:29, 22.15s/batch][A
Batches:  56%|█████▌    | 28/50 [10:21<08:07, 22.17s/batch][A
Batches:  58%|█████▊    | 29/50 [10:43<07:46, 22.21s/batch][A
Batches:  60%|██████    | 30/50 [11:05<07:23, 22.15s/batch][A
Batches:  62%|██████▏   | 31/50 [11:27<07:01, 22.18s/batch][A
Batches:  64%|██████▍   | 32/50 [11:50<06:39, 22.19s/batch][A
Batches:  66%|██████▌   | 33/50 [12:12<06:17, 22.21s/batch][A
Batches:  68%|██████▊   | 34/50 [12:34<05:54, 22.17s/batch][A
Batches:  70%|███████   | 35/50 [12:56<05:32, 22.19s/batch][A
Batches:  72%|███████▏  | 36/50 [13:18<05:10, 22.20s/batch][A
Batches:  74%|███████▍  | 37/50 [13:41<04:48, 22.22s/batch][A
Batches:  76%|███████▌  | 38/50 [14:03<04:26, 22.22s/batch][A
Batches:  78%|███████▊  | 39/50 [14:25<04:04, 22.22s/batch][A
Batches:  80%|████████  | 40/50 [14:47<03:42, 22.28s/batch][A
Batches:  82%|████████▏ | 41/50 [15:10<03:20, 22.26s/batch][A
Batches:  84%|████████▍ | 42/50 [15:32<02:57, 22.24s/batch][A
Batches:  86%|████████▌ | 43/50 [15:54<02:35, 22.19s/batch][A
Batches:  88%|████████▊ | 44/50 [16:16<02:13, 22.20s/batch][A
Batches:  90%|█████████ | 45/50 [16:38<01:50, 22.14s/batch][A
Batches:  92%|█████████▏| 46/50 [17:00<01:28, 22.13s/batch][A
Batches:  94%|█████████▍| 47/50 [17:22<01:06, 22.13s/batch][A
Batches:  96%|█████████▌| 48/50 [17:45<00:44, 22.23s/batch][A
Batches:  98%|█████████▊| 49/50 [18:07<00:22, 22.19s/batch][A
Batches: 100%|██████████| 50/50 [18:29<00:00, 22.21s/batch][A
                                                           [AEpochs:   1%|          | 2/200 [37:13<61:23:47, 1116.30s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:21<17:54, 21.92s/batch][A
Batches:   4%|▍         | 2/50 [00:43<17:35, 21.99s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:16, 22.05s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:54, 22.05s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:35, 22.12s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:14, 22.15s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:50, 22.11s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:27, 22.07s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:06, 22.10s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:44, 22.10s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:22, 22.11s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<13:59, 22.09s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:37, 22.10s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:16, 22.11s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:53, 22.09s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:29, 22.05s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:08, 22.06s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:45, 22.06s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:26, 22.14s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:04, 22.14s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:40, 22.09s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:19, 22.14s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:56, 22.10s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:35, 22.15s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:12, 22.11s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:50, 22.09s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:28, 22.09s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:06, 22.12s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:44, 22.11s/batch][A
Batches:  60%|██████    | 30/50 [11:03<07:23, 22.15s/batch][A
Batches:  62%|██████▏   | 31/50 [11:25<07:00, 22.12s/batch][A
Batches:  64%|██████▍   | 32/50 [11:47<06:38, 22.13s/batch][A
Batches:  66%|██████▌   | 33/50 [12:09<06:15, 22.09s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:54, 22.13s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:31, 22.13s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:10, 22.15s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:47, 22.10s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:24, 22.08s/batch][A
Batches:  78%|███████▊  | 39/50 [14:22<04:03, 22.10s/batch][A
Batches:  80%|████████  | 40/50 [14:44<03:41, 22.11s/batch][A
Batches:  82%|████████▏ | 41/50 [15:06<03:18, 22.08s/batch][A
Batches:  84%|████████▍ | 42/50 [15:28<02:56, 22.05s/batch][A
Batches:  86%|████████▌ | 43/50 [15:50<02:34, 22.09s/batch][A
Batches:  88%|████████▊ | 44/50 [16:12<02:12, 22.05s/batch][A
Batches:  90%|█████████ | 45/50 [16:34<01:50, 22.07s/batch][A
Batches:  92%|█████████▏| 46/50 [16:56<01:28, 22.05s/batch][A
Batches:  94%|█████████▍| 47/50 [17:18<01:06, 22.13s/batch][A
Batches:  96%|█████████▌| 48/50 [17:40<00:44, 22.08s/batch][A
Batches:  98%|█████████▊| 49/50 [18:02<00:22, 22.07s/batch][A
Batches: 100%|██████████| 50/50 [18:24<00:00, 22.12s/batch][A
                                                           [AEpochs:   2%|▏         | 3/200 [55:42<60:54:11, 1112.95s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:09, 22.23s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:40, 22.10s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:18, 22.10s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:59, 22.17s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:35, 22.12s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:11, 22.09s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:49, 22.07s/batch][A
Batches:  16%|█▌        | 8/50 [02:57<15:30, 22.14s/batch][A
Batches:  18%|█▊        | 9/50 [03:19<15:08, 22.17s/batch][A
Batches:  20%|██        | 10/50 [03:41<14:46, 22.17s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:22, 22.12s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<14:00, 22.13s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:38, 22.12s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:13, 22.05s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:52, 22.06s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:29, 22.05s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:06, 22.03s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:46, 22.08s/batch][A
Batches:  38%|███▊      | 19/50 [07:00<11:26, 22.14s/batch][A
Batches:  40%|████      | 20/50 [07:22<11:03, 22.11s/batch][A
Batches:  42%|████▏     | 21/50 [07:44<10:41, 22.12s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:19, 22.11s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:58, 22.15s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:33, 22.07s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:12, 22.11s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:49, 22.07s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:29, 22.14s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:05, 22.05s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:43, 22.05s/batch][A
Batches:  60%|██████    | 30/50 [11:03<07:22, 22.12s/batch][A
Batches:  62%|██████▏   | 31/50 [11:25<06:59, 22.09s/batch][A
Batches:  64%|██████▍   | 32/50 [11:47<06:37, 22.06s/batch][A
Batches:  66%|██████▌   | 33/50 [12:09<06:14, 22.06s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:53, 22.10s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:31, 22.07s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:08, 22.03s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:47, 22.15s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:25, 22.11s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:03, 22.13s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:40, 22.09s/batch][A
Batches:  82%|████████▏ | 41/50 [15:06<03:19, 22.14s/batch][A
Batches:  84%|████████▍ | 42/50 [15:28<02:56, 22.09s/batch][A
Batches:  86%|████████▌ | 43/50 [15:50<02:34, 22.06s/batch][A
Batches:  88%|████████▊ | 44/50 [16:12<02:12, 22.06s/batch][A
Batches:  90%|█████████ | 45/50 [16:34<01:50, 22.12s/batch][A
Batches:  92%|█████████▏| 46/50 [16:56<01:28, 22.12s/batch][A
Batches:  94%|█████████▍| 47/50 [17:18<01:06, 22.11s/batch][A
Batches:  96%|█████████▌| 48/50 [17:40<00:44, 22.13s/batch][A
Batches:  98%|█████████▊| 49/50 [18:02<00:22, 22.12s/batch][A
Batches: 100%|██████████| 50/50 [18:25<00:00, 22.11s/batch][A
                                                           [AEpochs:   2%|▏         | 4/200 [1:14:11<60:30:33, 1111.39s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:00, 22.05s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:40, 22.10s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:20, 22.13s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:56, 22.09s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:33, 22.08s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:10, 22.05s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:51, 22.12s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:29, 22.12s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:07, 22.12s/batch][A
Batches:  20%|██        | 10/50 [03:41<14:47, 22.19s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:25, 22.19s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<14:01, 22.15s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:38, 22.12s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:16, 22.13s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:50, 22.03s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:28, 22.01s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:06, 22.00s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:45, 22.05s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:24, 22.08s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:01, 22.04s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:39, 22.06s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:18, 22.09s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:56, 22.09s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:34, 22.08s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:11, 22.06s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:49, 22.06s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:26, 22.03s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:05, 22.06s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:44, 22.12s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:21, 22.08s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:59, 22.08s/batch][A
Batches:  64%|██████▍   | 32/50 [11:46<06:37, 22.08s/batch][A
Batches:  66%|██████▌   | 33/50 [12:08<06:16, 22.14s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:53, 22.10s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:31, 22.12s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:09, 22.12s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:47, 22.10s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:25, 22.08s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:02, 22.09s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:41, 22.13s/batch][A
Batches:  82%|████████▏ | 41/50 [15:05<03:19, 22.12s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:57, 22.13s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.04s/batch][A
Batches:  88%|████████▊ | 44/50 [16:11<02:12, 22.03s/batch][A
Batches:  90%|█████████ | 45/50 [16:33<01:50, 22.03s/batch][A
Batches:  92%|█████████▏| 46/50 [16:55<01:28, 22.00s/batch][A
Batches:  94%|█████████▍| 47/50 [17:17<01:05, 21.99s/batch][A
Batches:  96%|█████████▌| 48/50 [17:39<00:44, 22.06s/batch][A
Batches:  98%|█████████▊| 49/50 [18:02<00:22, 22.09s/batch][A
Batches: 100%|██████████| 50/50 [18:23<00:00, 22.02s/batch][A
                                                           [AEpochs:   2%|▎         | 5/200 [1:32:39<60:08:08, 1110.19s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:21<17:57, 22.00s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:41, 22.12s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:18, 22.11s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:53, 22.03s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:35, 22.13s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:12, 22.10s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:48, 22.07s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:26, 22.05s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:03, 22.04s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:40, 22.02s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:17, 21.98s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:56, 22.02s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:46, 22.33s/batch][A
Batches:  28%|██▊       | 14/50 [05:10<13:29, 22.50s/batch][A
Batches:  30%|███       | 15/50 [05:33<13:15, 22.72s/batch][A
Batches:  32%|███▏      | 16/50 [05:56<12:56, 22.84s/batch][A
Batches:  34%|███▍      | 17/50 [06:20<12:37, 22.94s/batch][A
Batches:  36%|███▌      | 18/50 [06:43<12:15, 22.98s/batch][A
Batches:  38%|███▊      | 19/50 [07:06<11:53, 23.01s/batch][A
Batches:  40%|████      | 20/50 [07:28<11:24, 22.82s/batch][A
Batches:  42%|████▏     | 21/50 [07:50<10:56, 22.62s/batch][A
Batches:  44%|████▍     | 22/50 [08:12<10:28, 22.44s/batch][A
Batches:  46%|████▌     | 23/50 [08:34<10:03, 22.36s/batch][A
Batches:  48%|████▊     | 24/50 [08:57<09:39, 22.29s/batch][A
Batches:  50%|█████     | 25/50 [09:19<09:15, 22.24s/batch][A
Batches:  52%|█████▏    | 26/50 [09:41<08:51, 22.16s/batch][A
Batches:  54%|█████▍    | 27/50 [10:03<08:29, 22.17s/batch][A
Batches:  56%|█████▌    | 28/50 [10:25<08:07, 22.16s/batch][A
Batches:  58%|█████▊    | 29/50 [10:47<07:44, 22.13s/batch][A
Batches:  60%|██████    | 30/50 [11:09<07:22, 22.10s/batch][A
Batches:  62%|██████▏   | 31/50 [11:31<07:00, 22.12s/batch][A
Batches:  64%|██████▍   | 32/50 [11:54<06:39, 22.17s/batch][A
Batches:  66%|██████▌   | 33/50 [12:16<06:16, 22.16s/batch][A
Batches:  68%|██████▊   | 34/50 [12:38<05:53, 22.12s/batch][A
Batches:  70%|███████   | 35/50 [13:00<05:32, 22.16s/batch][A
Batches:  72%|███████▏  | 36/50 [13:22<05:10, 22.15s/batch][A
Batches:  74%|███████▍  | 37/50 [13:44<04:47, 22.15s/batch][A
Batches:  76%|███████▌  | 38/50 [14:06<04:25, 22.14s/batch][A
Batches:  78%|███████▊  | 39/50 [14:29<04:03, 22.16s/batch][A
Batches:  80%|████████  | 40/50 [14:51<03:41, 22.12s/batch][A
Batches:  82%|████████▏ | 41/50 [15:13<03:18, 22.10s/batch][A
Batches:  84%|████████▍ | 42/50 [15:35<02:56, 22.10s/batch][A
Batches:  86%|████████▌ | 43/50 [15:57<02:34, 22.12s/batch][A
Batches:  88%|████████▊ | 44/50 [16:19<02:12, 22.05s/batch][A
Batches:  90%|█████████ | 45/50 [16:41<01:50, 22.06s/batch][A
Batches:  92%|█████████▏| 46/50 [17:03<01:28, 22.09s/batch][A
Batches:  94%|█████████▍| 47/50 [17:25<01:06, 22.11s/batch][A
Batches:  96%|█████████▌| 48/50 [17:47<00:44, 22.12s/batch][A
Batches:  98%|█████████▊| 49/50 [18:10<00:22, 22.14s/batch][A
Batches: 100%|██████████| 50/50 [18:32<00:00, 22.17s/batch][A
                                                           [AEpochs:   3%|▎         | 6/200 [1:51:16<59:56:23, 1112.29s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:05, 22.16s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:40, 22.10s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:15, 22.03s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:56, 22.09s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:32, 22.05s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:08, 22.01s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:47, 22.03s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:28, 22.12s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:04, 22.07s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:43, 22.09s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:21, 22.09s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:59, 22.10s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:37, 22.08s/batch][A
Batches:  28%|██▊       | 14/50 [05:08<13:13, 22.04s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:52, 22.06s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:29, 22.05s/batch][A
Batches:  34%|███▍      | 17/50 [06:14<12:06, 22.00s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:44, 22.02s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:23, 22.04s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:00, 22.00s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:38, 22.03s/batch][A
Batches:  44%|████▍     | 22/50 [08:05<10:16, 22.02s/batch][A
Batches:  46%|████▌     | 23/50 [08:27<09:57, 22.12s/batch][A
Batches:  48%|████▊     | 24/50 [08:49<09:35, 22.13s/batch][A
Batches:  50%|█████     | 25/50 [09:11<09:11, 22.08s/batch][A
Batches:  52%|█████▏    | 26/50 [09:33<08:50, 22.11s/batch][A
Batches:  54%|█████▍    | 27/50 [09:55<08:28, 22.12s/batch][A
Batches:  56%|█████▌    | 28/50 [10:17<08:06, 22.10s/batch][A
Batches:  58%|█████▊    | 29/50 [10:39<07:43, 22.06s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:21, 22.09s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:59, 22.06s/batch][A
Batches:  64%|██████▍   | 32/50 [11:46<06:37, 22.08s/batch][A
Batches:  66%|██████▌   | 33/50 [12:08<06:15, 22.09s/batch][A
Batches:  68%|██████▊   | 34/50 [12:30<05:53, 22.11s/batch][A
Batches:  70%|███████   | 35/50 [12:52<05:30, 22.04s/batch][A
Batches:  72%|███████▏  | 36/50 [13:14<05:08, 22.04s/batch][A
Batches:  74%|███████▍  | 37/50 [13:36<04:46, 22.04s/batch][A
Batches:  76%|███████▌  | 38/50 [13:58<04:24, 22.08s/batch][A
Batches:  78%|███████▊  | 39/50 [14:20<04:02, 22.05s/batch][A
Batches:  80%|████████  | 40/50 [14:42<03:40, 22.01s/batch][A
Batches:  82%|████████▏ | 41/50 [15:04<03:18, 22.02s/batch][A
Batches:  84%|████████▍ | 42/50 [15:26<02:56, 22.04s/batch][A
Batches:  86%|████████▌ | 43/50 [15:48<02:33, 21.99s/batch][A
Batches:  88%|████████▊ | 44/50 [16:10<02:12, 22.02s/batch][A
Batches:  90%|█████████ | 45/50 [16:32<01:50, 22.00s/batch][A
Batches:  92%|█████████▏| 46/50 [16:54<01:28, 22.01s/batch][A
Batches:  94%|█████████▍| 47/50 [17:16<01:06, 22.02s/batch][A
Batches:  96%|█████████▌| 48/50 [17:38<00:44, 22.04s/batch][A
Batches:  98%|█████████▊| 49/50 [18:00<00:22, 22.05s/batch][A
Batches: 100%|██████████| 50/50 [18:22<00:00, 22.04s/batch][A
                                                           [AEpochs:   4%|▎         | 7/200 [2:09:42<59:32:07, 1110.50s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<17:59, 22.02s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:41, 22.11s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:16, 22.06s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:56, 22.10s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:33, 22.08s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:10, 22.07s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:48, 22.05s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:25, 22.04s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:04, 22.06s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:44, 22.11s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:19, 22.05s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:58, 22.05s/batch][A
Batches:  26%|██▌       | 13/50 [04:46<13:37, 22.11s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:15, 22.08s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:54, 22.12s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:32, 22.13s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:12, 22.20s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:49, 22.19s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:25, 22.12s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:02, 22.08s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:40, 22.09s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:18, 22.10s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:56, 22.10s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:34, 22.10s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:13, 22.12s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:50, 22.11s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:27, 22.05s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:04, 22.04s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:43, 22.08s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:22, 22.11s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:59, 22.07s/batch][A
Batches:  64%|██████▍   | 32/50 [11:46<06:37, 22.07s/batch][A
Batches:  66%|██████▌   | 33/50 [12:08<06:15, 22.08s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:53, 22.07s/batch][A
Batches:  70%|███████   | 35/50 [12:52<05:30, 22.00s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:08, 22.07s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:46, 22.07s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:24, 22.06s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:02, 22.04s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:41, 22.13s/batch][A
Batches:  82%|████████▏ | 41/50 [15:05<03:18, 22.10s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:57, 22.13s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.08s/batch][A
Batches:  88%|████████▊ | 44/50 [16:11<02:12, 22.10s/batch][A
Batches:  90%|█████████ | 45/50 [16:33<01:50, 22.07s/batch][A
Batches:  92%|█████████▏| 46/50 [16:55<01:28, 22.02s/batch][A
Batches:  94%|█████████▍| 47/50 [17:17<01:06, 22.06s/batch][A
Batches:  96%|█████████▌| 48/50 [17:40<00:44, 22.07s/batch][A
Batches:  98%|█████████▊| 49/50 [18:02<00:22, 22.05s/batch][A
Batches: 100%|██████████| 50/50 [18:24<00:00, 22.04s/batch][A
                                                           [AEpochs:   4%|▍         | 8/200 [2:28:11<59:11:14, 1109.76s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:03, 22.11s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:47, 22.25s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:21, 22.17s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:55, 22.08s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:35, 22.12s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:12, 22.09s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:46, 22.01s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:25, 22.03s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:07, 22.13s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:42, 22.07s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:20, 22.06s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:56, 22.01s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:37, 22.09s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:15, 22.08s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:52, 22.07s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:31, 22.11s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:08, 22.07s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:46, 22.07s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:23, 22.04s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:01, 22.07s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:39, 22.04s/batch][A
Batches:  44%|████▍     | 22/50 [08:05<10:17, 22.05s/batch][A
Batches:  46%|████▌     | 23/50 [08:27<09:55, 22.06s/batch][A
Batches:  48%|████▊     | 24/50 [08:49<09:34, 22.10s/batch][A
Batches:  50%|█████     | 25/50 [09:11<09:11, 22.07s/batch][A
Batches:  52%|█████▏    | 26/50 [09:33<08:49, 22.05s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:27, 22.08s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:06, 22.09s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:43, 22.08s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:21, 22.09s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:59, 22.07s/batch][A
Batches:  64%|██████▍   | 32/50 [11:46<06:37, 22.08s/batch][A
Batches:  66%|██████▌   | 33/50 [12:08<06:15, 22.10s/batch][A
Batches:  68%|██████▊   | 34/50 [12:30<05:52, 22.04s/batch][A
Batches:  70%|███████   | 35/50 [12:52<05:30, 22.03s/batch][A
Batches:  72%|███████▏  | 36/50 [13:14<05:09, 22.08s/batch][A
Batches:  74%|███████▍  | 37/50 [13:36<04:46, 22.05s/batch][A
Batches:  76%|███████▌  | 38/50 [13:58<04:24, 22.03s/batch][A
Batches:  78%|███████▊  | 39/50 [14:20<04:02, 22.09s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:41, 22.11s/batch][A
Batches:  82%|████████▏ | 41/50 [15:05<03:19, 22.13s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:57, 22.14s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.14s/batch][A
Batches:  88%|████████▊ | 44/50 [16:11<02:12, 22.12s/batch][A
Batches:  90%|█████████ | 45/50 [16:33<01:50, 22.08s/batch][A
Batches:  92%|█████████▏| 46/50 [16:55<01:28, 22.03s/batch][A
Batches:  94%|█████████▍| 47/50 [17:17<01:06, 22.06s/batch][A
Batches:  96%|█████████▌| 48/50 [17:39<00:44, 22.04s/batch][A
Batches:  98%|█████████▊| 49/50 [18:01<00:22, 22.04s/batch][A
Batches: 100%|██████████| 50/50 [18:23<00:00, 22.04s/batch][A
                                                           [AEpochs:   4%|▍         | 9/200 [2:46:38<58:50:43, 1109.13s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:10, 22.26s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:44, 22.17s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:16, 22.05s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:55, 22.08s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:31, 22.04s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:09, 22.03s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:48, 22.05s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:26, 22.06s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:05, 22.09s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:43, 22.10s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:21, 22.08s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<14:00, 22.11s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:38, 22.13s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:15, 22.11s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:53, 22.11s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:31, 22.11s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:09, 22.10s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:47, 22.10s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:24, 22.07s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:03, 22.12s/batch][A
Batches:  42%|████▏     | 21/50 [07:44<10:41, 22.13s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:18, 22.10s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:55, 22.06s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:33, 22.05s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:11, 22.05s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:48, 22.03s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:27, 22.08s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:06, 22.10s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:42, 22.04s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:21, 22.05s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<07:00, 22.13s/batch][A
Batches:  64%|██████▍   | 32/50 [11:47<06:38, 22.16s/batch][A
Batches:  66%|██████▌   | 33/50 [12:09<06:16, 22.12s/batch][A
Batches:  68%|██████▊   | 34/50 [12:30<05:52, 22.05s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:31, 22.10s/batch][A
Batches:  72%|███████▏  | 36/50 [13:14<05:08, 22.02s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:46, 22.05s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:24, 22.07s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:02, 22.08s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:40, 22.06s/batch][A
Batches:  82%|████████▏ | 41/50 [15:05<03:18, 22.04s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:56, 22.00s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.08s/batch][A
Batches:  88%|████████▊ | 44/50 [16:11<02:12, 22.06s/batch][A
Batches:  90%|█████████ | 45/50 [16:33<01:50, 22.02s/batch][A
Batches:  92%|█████████▏| 46/50 [16:55<01:28, 22.04s/batch][A
Batches:  94%|█████████▍| 47/50 [17:17<01:06, 22.03s/batch][A
Batches:  96%|█████████▌| 48/50 [17:39<00:43, 22.00s/batch][A
Batches:  98%|█████████▊| 49/50 [18:01<00:22, 22.04s/batch][A
Batches: 100%|██████████| 50/50 [18:23<00:00, 22.08s/batch][A
                                                           [AEpochs:   5%|▌         | 10/200 [3:05:06<58:30:51, 1108.69s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<17:59, 22.04s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:38, 22.05s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:20, 22.15s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:58, 22.14s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:34, 22.10s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:09, 22.03s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:50, 22.11s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:28, 22.10s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:05, 22.08s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:44, 22.11s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:23, 22.14s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<13:59, 22.09s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:37, 22.08s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:16, 22.13s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:54, 22.13s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:32, 22.14s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:10, 22.14s/batch][A
Batches:  36%|███▌      | 18/50 [06:38<11:49, 22.17s/batch][A
Batches:  38%|███▊      | 19/50 [07:00<11:26, 22.13s/batch][A
Batches:  40%|████      | 20/50 [07:22<11:02, 22.09s/batch][A
Batches:  42%|████▏     | 21/50 [07:44<10:41, 22.13s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:21, 22.21s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:59, 22.20s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:34, 22.11s/batch][A
Batches:  50%|█████     | 25/50 [09:13<09:14, 22.20s/batch][A
Batches:  52%|█████▏    | 26/50 [09:35<08:51, 22.15s/batch][A
Batches:  54%|█████▍    | 27/50 [09:57<08:28, 22.12s/batch][A
Batches:  56%|█████▌    | 28/50 [10:19<08:07, 22.16s/batch][A
Batches:  58%|█████▊    | 29/50 [10:41<07:46, 22.22s/batch][A
Batches:  60%|██████    | 30/50 [11:04<07:23, 22.20s/batch][A
Batches:  62%|██████▏   | 31/50 [11:26<07:00, 22.15s/batch][A
Batches:  64%|██████▍   | 32/50 [11:48<06:38, 22.16s/batch][A
Batches:  66%|██████▌   | 33/50 [12:10<06:16, 22.17s/batch][A
Batches:  68%|██████▊   | 34/50 [12:32<05:54, 22.13s/batch][A
Batches:  70%|███████   | 35/50 [12:54<05:31, 22.10s/batch][A
Batches:  72%|███████▏  | 36/50 [13:16<05:09, 22.10s/batch][A
Batches:  74%|███████▍  | 37/50 [13:38<04:47, 22.14s/batch][A
Batches:  76%|███████▌  | 38/50 [14:01<04:25, 22.14s/batch][A
Batches:  78%|███████▊  | 39/50 [14:23<04:02, 22.08s/batch][A
Batches:  80%|████████  | 40/50 [14:45<03:41, 22.15s/batch][A
Batches:  82%|████████▏ | 41/50 [15:07<03:19, 22.12s/batch][A
Batches:  84%|████████▍ | 42/50 [15:29<02:56, 22.10s/batch][A
Batches:  86%|████████▌ | 43/50 [15:51<02:35, 22.15s/batch][A
Batches:  88%|████████▊ | 44/50 [16:13<02:12, 22.13s/batch][A
Batches:  90%|█████████ | 45/50 [16:35<01:50, 22.11s/batch][A
Batches:  92%|█████████▏| 46/50 [16:57<01:28, 22.11s/batch][A
Batches:  94%|█████████▍| 47/50 [17:20<01:06, 22.10s/batch][A
Batches:  96%|█████████▌| 48/50 [17:42<00:44, 22.14s/batch][A
Batches:  98%|█████████▊| 49/50 [18:04<00:22, 22.09s/batch][A
Batches: 100%|██████████| 50/50 [18:26<00:00, 22.07s/batch][A
                                                           [AEpochs:   6%|▌         | 11/200 [3:23:36<58:13:53, 1109.17s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:02, 22.08s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:45, 22.19s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:18, 22.10s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:56, 22.09s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:38, 22.18s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:15, 22.16s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:50, 22.11s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:26, 22.07s/batch][A
Batches:  18%|█▊        | 9/50 [03:19<15:05, 22.09s/batch][A
Batches:  20%|██        | 10/50 [03:41<14:44, 22.10s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:22, 22.12s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<13:59, 22.10s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:38, 22.11s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:15, 22.11s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:53, 22.09s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:29, 22.05s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:08, 22.07s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:46, 22.07s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:23, 22.06s/batch][A
Batches:  40%|████      | 20/50 [07:22<11:02, 22.10s/batch][A
Batches:  42%|████▏     | 21/50 [07:44<10:41, 22.12s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:19, 22.12s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:57, 22.12s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:35, 22.12s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:12, 22.10s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:50, 22.09s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:28, 22.12s/batch][A
Batches:  56%|█████▌    | 28/50 [10:19<08:07, 22.14s/batch][A
Batches:  58%|█████▊    | 29/50 [10:41<07:43, 22.09s/batch][A
Batches:  60%|██████    | 30/50 [11:03<07:21, 22.09s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:58, 22.03s/batch][A
Batches:  64%|██████▍   | 32/50 [11:47<06:37, 22.08s/batch][A
Batches:  66%|██████▌   | 33/50 [12:09<06:15, 22.09s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:53, 22.09s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:32, 22.16s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:10, 22.16s/batch][A
Batches:  74%|███████▍  | 37/50 [13:38<04:47, 22.15s/batch][A
Batches:  76%|███████▌  | 38/50 [14:00<04:25, 22.12s/batch][A
Batches:  78%|███████▊  | 39/50 [14:22<04:03, 22.14s/batch][A
Batches:  80%|████████  | 40/50 [14:44<03:41, 22.10s/batch][A
Batches:  82%|████████▏ | 41/50 [15:06<03:18, 22.05s/batch][A
Batches:  84%|████████▍ | 42/50 [15:28<02:56, 22.04s/batch][A
Batches:  86%|████████▌ | 43/50 [15:50<02:34, 22.08s/batch][A
Batches:  88%|████████▊ | 44/50 [16:12<02:12, 22.12s/batch][A
Batches:  90%|█████████ | 45/50 [16:34<01:50, 22.10s/batch][A
Batches:  92%|█████████▏| 46/50 [16:56<01:28, 22.06s/batch][A
Batches:  94%|█████████▍| 47/50 [17:18<01:06, 22.12s/batch][A
Batches:  96%|█████████▌| 48/50 [17:41<00:44, 22.13s/batch][A
Batches:  98%|█████████▊| 49/50 [18:03<00:22, 22.10s/batch][A
Batches: 100%|██████████| 50/50 [18:25<00:00, 22.07s/batch][A
                                                           [AEpochs:   6%|▌         | 12/200 [3:42:05<57:55:20, 1109.15s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:00, 22.06s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:39, 22.07s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:16, 22.06s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:58, 22.15s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:37, 22.16s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:16, 22.19s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:51, 22.13s/batch][A
Batches:  16%|█▌        | 8/50 [02:57<15:31, 22.17s/batch][A
Batches:  18%|█▊        | 9/50 [03:19<15:08, 22.16s/batch][A
Batches:  20%|██        | 10/50 [03:41<14:45, 22.15s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:20, 22.07s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<13:59, 22.10s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:37, 22.10s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:14, 22.07s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:51, 22.04s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:32, 22.12s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:10, 22.13s/batch][A
Batches:  36%|███▌      | 18/50 [06:38<11:47, 22.11s/batch][A
Batches:  38%|███▊      | 19/50 [07:00<11:24, 22.07s/batch][A
Batches:  40%|████      | 20/50 [07:22<11:03, 22.12s/batch][A
Batches:  42%|████▏     | 21/50 [07:44<10:39, 22.04s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:18, 22.08s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:58, 22.15s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:34, 22.11s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:12, 22.10s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:48, 22.03s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:27, 22.08s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:05, 22.07s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:43, 22.09s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:21, 22.05s/batch][A
Batches:  62%|██████▏   | 31/50 [11:25<07:00, 22.11s/batch][A
Batches:  64%|██████▍   | 32/50 [11:47<06:37, 22.07s/batch][A
Batches:  66%|██████▌   | 33/50 [12:09<06:15, 22.07s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:52, 22.02s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:30, 22.06s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:08, 22.03s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:46, 22.03s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:25, 22.12s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:03, 22.18s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:41, 22.14s/batch][A
Batches:  82%|████████▏ | 41/50 [15:06<03:19, 22.13s/batch][A
Batches:  84%|████████▍ | 42/50 [15:28<02:57, 22.16s/batch][A
Batches:  86%|████████▌ | 43/50 [15:50<02:35, 22.16s/batch][A
Batches:  88%|████████▊ | 44/50 [16:12<02:12, 22.16s/batch][A
Batches:  90%|█████████ | 45/50 [16:34<01:50, 22.12s/batch][A
Batches:  92%|█████████▏| 46/50 [16:56<01:28, 22.19s/batch][A
Batches:  94%|█████████▍| 47/50 [17:19<01:06, 22.18s/batch][A
Batches:  96%|█████████▌| 48/50 [17:41<00:44, 22.12s/batch][A
Batches:  98%|█████████▊| 49/50 [18:03<00:22, 22.11s/batch][A
Batches: 100%|██████████| 50/50 [18:25<00:00, 22.10s/batch][A
                                                           [AEpochs:   6%|▋         | 13/200 [4:00:35<57:37:07, 1109.24s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:04, 22.13s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:37, 22.04s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:18, 22.10s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:54, 22.06s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:34, 22.09s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:11, 22.07s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:51, 22.13s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:28, 22.10s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:06, 22.12s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:44, 22.10s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:23, 22.13s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<14:00, 22.12s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:37, 22.10s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:18, 22.19s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:55, 22.16s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:33, 22.15s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:09, 22.12s/batch][A
Batches:  36%|███▌      | 18/50 [06:38<11:47, 22.10s/batch][A
Batches:  38%|███▊      | 19/50 [07:00<11:24, 22.06s/batch][A
Batches:  40%|████      | 20/50 [07:22<11:02, 22.08s/batch][A
Batches:  42%|████▏     | 21/50 [07:44<10:40, 22.10s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:18, 22.09s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:56, 22.11s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:33, 22.06s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:12, 22.09s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:50, 22.09s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:27, 22.07s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:05, 22.08s/batch][A
Batches:  58%|█████▊    | 29/50 [10:41<07:45, 22.16s/batch][A
Batches:  60%|██████    | 30/50 [11:03<07:22, 22.14s/batch][A
Batches:  62%|██████▏   | 31/50 [11:25<07:00, 22.12s/batch][A
Batches:  64%|██████▍   | 32/50 [11:47<06:37, 22.08s/batch][A
Batches:  66%|██████▌   | 33/50 [12:09<06:16, 22.12s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:53, 22.08s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:30, 22.06s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:08, 22.02s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:46, 22.06s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:24, 22.06s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:02, 22.05s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:40, 22.01s/batch][A
Batches:  82%|████████▏ | 41/50 [15:05<03:18, 22.06s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:56, 22.06s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.06s/batch][A
Batches:  88%|████████▊ | 44/50 [16:12<02:12, 22.11s/batch][A
Batches:  90%|█████████ | 45/50 [16:34<01:50, 22.10s/batch][A
Batches:  92%|█████████▏| 46/50 [16:56<01:28, 22.09s/batch][A
Batches:  94%|█████████▍| 47/50 [17:18<01:06, 22.11s/batch][A
Batches:  96%|█████████▌| 48/50 [17:40<00:44, 22.12s/batch][A
Batches:  98%|█████████▊| 49/50 [18:02<00:22, 22.11s/batch][A
Batches: 100%|██████████| 50/50 [18:24<00:00, 22.12s/batch][A
                                                           [AEpochs:   7%|▋         | 14/200 [4:19:04<57:18:17, 1109.13s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<17:59, 22.03s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:41, 22.11s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:19, 22.11s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:56, 22.10s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:36, 22.14s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:13, 22.14s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:50, 22.11s/batch][A
Batches:  16%|█▌        | 8/50 [02:57<15:31, 22.18s/batch][A
Batches:  18%|█▊        | 9/50 [03:19<15:09, 22.18s/batch][A
Batches:  20%|██        | 10/50 [03:41<14:46, 22.17s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:24, 22.17s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<14:01, 22.15s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:39, 22.16s/batch][A
Batches:  28%|██▊       | 14/50 [05:10<13:17, 22.15s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:52, 22.07s/batch][A
Batches:  32%|███▏      | 16/50 [05:54<12:32, 22.13s/batch][A
Batches:  34%|███▍      | 17/50 [06:16<12:09, 22.11s/batch][A
Batches:  36%|███▌      | 18/50 [06:38<11:48, 22.15s/batch][A
Batches:  38%|███▊      | 19/50 [07:00<11:26, 22.15s/batch][A
Batches:  40%|████      | 20/50 [07:22<11:05, 22.19s/batch][A
Batches:  42%|████▏     | 21/50 [07:45<10:42, 22.17s/batch][A
Batches:  44%|████▍     | 22/50 [08:07<10:19, 22.14s/batch][A
Batches:  46%|████▌     | 23/50 [08:29<09:58, 22.17s/batch][A
Batches:  48%|████▊     | 24/50 [08:51<09:35, 22.12s/batch][A
Batches:  50%|█████     | 25/50 [09:13<09:13, 22.12s/batch][A
Batches:  52%|█████▏    | 26/50 [09:35<08:50, 22.10s/batch][A
Batches:  54%|█████▍    | 27/50 [09:57<08:28, 22.11s/batch][A
Batches:  56%|█████▌    | 28/50 [10:19<08:05, 22.06s/batch][A
Batches:  58%|█████▊    | 29/50 [10:41<07:43, 22.07s/batch][A
Batches:  60%|██████    | 30/50 [11:03<07:22, 22.10s/batch][A
Batches:  62%|██████▏   | 31/50 [11:26<07:00, 22.15s/batch][A
Batches:  64%|██████▍   | 32/50 [11:48<06:37, 22.07s/batch][A
Batches:  66%|██████▌   | 33/50 [12:10<06:15, 22.08s/batch][A
Batches:  68%|██████▊   | 34/50 [12:32<05:52, 22.03s/batch][A
Batches:  70%|███████   | 35/50 [12:54<05:30, 22.04s/batch][A
Batches:  72%|███████▏  | 36/50 [13:16<05:08, 22.05s/batch][A
Batches:  74%|███████▍  | 37/50 [13:38<04:46, 22.06s/batch][A
Batches:  76%|███████▌  | 38/50 [14:00<04:24, 22.07s/batch][A
Batches:  78%|███████▊  | 39/50 [14:22<04:02, 22.07s/batch][A
Batches:  80%|████████  | 40/50 [14:44<03:40, 22.03s/batch][A
Batches:  82%|████████▏ | 41/50 [15:06<03:18, 22.04s/batch][A
Batches:  84%|████████▍ | 42/50 [15:28<02:56, 22.05s/batch][A
Batches:  86%|████████▌ | 43/50 [15:50<02:34, 22.01s/batch][A
Batches:  88%|████████▊ | 44/50 [16:12<02:12, 22.07s/batch][A
Batches:  90%|█████████ | 45/50 [16:34<01:50, 22.05s/batch][A
Batches:  92%|█████████▏| 46/50 [16:56<01:28, 22.09s/batch][A
Batches:  94%|█████████▍| 47/50 [17:19<01:06, 22.13s/batch][A
Batches:  96%|█████████▌| 48/50 [17:41<00:44, 22.08s/batch][A
Batches:  98%|█████████▊| 49/50 [18:03<00:22, 22.12s/batch][A
Batches: 100%|██████████| 50/50 [18:25<00:00, 22.17s/batch][A
                                                           [AEpochs:   8%|▊         | 15/200 [4:37:33<57:00:08, 1109.23s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:21<17:53, 21.91s/batch][A
Batches:   4%|▍         | 2/50 [00:43<17:34, 21.96s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:18, 22.09s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:56, 22.10s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:32, 22.05s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:09, 22.04s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:48, 22.06s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:24, 22.02s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:03, 22.04s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:41, 22.04s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:20, 22.06s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:58, 22.07s/batch][A
Batches:  26%|██▌       | 13/50 [04:46<13:36, 22.06s/batch][A
Batches:  28%|██▊       | 14/50 [05:08<13:12, 22.01s/batch][A
Batches:  30%|███       | 15/50 [05:30<12:52, 22.08s/batch][A
Batches:  32%|███▏      | 16/50 [05:52<12:29, 22.03s/batch][A
Batches:  34%|███▍      | 17/50 [06:14<12:06, 22.03s/batch][A
Batches:  36%|███▌      | 18/50 [06:36<11:47, 22.10s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:24, 22.08s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:02, 22.07s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:39, 22.06s/batch][A
Batches:  44%|████▍     | 22/50 [08:05<10:19, 22.11s/batch][A
Batches:  46%|████▌     | 23/50 [08:27<09:58, 22.15s/batch][A
Batches:  48%|████▊     | 24/50 [08:49<09:34, 22.10s/batch][A
Batches:  50%|█████     | 25/50 [09:11<09:11, 22.07s/batch][A
Batches:  52%|█████▏    | 26/50 [09:33<08:50, 22.08s/batch][A
Batches:  54%|█████▍    | 27/50 [09:55<08:26, 22.03s/batch][A
Batches:  56%|█████▌    | 28/50 [10:17<08:04, 22.02s/batch][A
Batches:  58%|█████▊    | 29/50 [10:39<07:43, 22.09s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:22, 22.13s/batch][A
Batches:  62%|██████▏   | 31/50 [11:23<06:58, 22.05s/batch][A
Batches:  64%|██████▍   | 32/50 [11:45<06:36, 22.03s/batch][A
Batches:  66%|██████▌   | 33/50 [12:07<06:14, 22.00s/batch][A
Batches:  68%|██████▊   | 34/50 [12:29<05:52, 22.03s/batch][A
Batches:  70%|███████   | 35/50 [12:51<05:30, 22.03s/batch][A
Batches:  72%|███████▏  | 36/50 [13:14<05:08, 22.04s/batch][A
Batches:  74%|███████▍  | 37/50 [13:35<04:46, 22.01s/batch][A
Batches:  76%|███████▌  | 38/50 [13:58<04:24, 22.05s/batch][A
Batches:  78%|███████▊  | 39/50 [14:20<04:02, 22.02s/batch][A
Batches:  80%|████████  | 40/50 [14:42<03:40, 22.07s/batch][A
Batches:  82%|████████▏ | 41/50 [15:04<03:18, 22.10s/batch][A
Batches:  84%|████████▍ | 42/50 [15:26<02:56, 22.10s/batch][A
Batches:  86%|████████▌ | 43/50 [15:48<02:34, 22.08s/batch][A
Batches:  88%|████████▊ | 44/50 [16:10<02:12, 22.10s/batch][A
Batches:  90%|█████████ | 45/50 [16:32<01:50, 22.12s/batch][A
Batches:  92%|█████████▏| 46/50 [16:54<01:28, 22.08s/batch][A
Batches:  94%|█████████▍| 47/50 [17:16<01:06, 22.09s/batch][A
Batches:  96%|█████████▌| 48/50 [17:38<00:44, 22.07s/batch][A
Batches:  98%|█████████▊| 49/50 [18:00<00:22, 22.00s/batch][A
Batches: 100%|██████████| 50/50 [18:22<00:00, 22.01s/batch][A
                                                           [AEpochs:   8%|▊         | 16/200 [4:56:00<56:39:33, 1108.55s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:04, 22.13s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:43, 22.15s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:19, 22.11s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:55, 22.07s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:30, 22.02s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:11, 22.07s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:49, 22.08s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:27, 22.08s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:05, 22.09s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:44, 22.11s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:20, 22.07s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:58, 22.07s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:40, 22.16s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:15, 22.09s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:50, 22.02s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:29, 22.04s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:07, 22.03s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:43, 21.97s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:22, 22.00s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:00, 22.03s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:38, 22.03s/batch][A
Batches:  44%|████▍     | 22/50 [08:05<10:16, 22.00s/batch][A
Batches:  46%|████▌     | 23/50 [08:27<09:54, 22.04s/batch][A
Batches:  48%|████▊     | 24/50 [08:49<09:34, 22.09s/batch][A
Batches:  50%|█████     | 25/50 [09:11<09:11, 22.07s/batch][A
Batches:  52%|█████▏    | 26/50 [09:33<08:49, 22.06s/batch][A
Batches:  54%|█████▍    | 27/50 [09:55<08:27, 22.04s/batch][A
Batches:  56%|█████▌    | 28/50 [10:17<08:06, 22.10s/batch][A
Batches:  58%|█████▊    | 29/50 [10:39<07:44, 22.11s/batch][A
Batches:  60%|██████    | 30/50 [11:01<07:21, 22.08s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:59, 22.09s/batch][A
Batches:  64%|██████▍   | 32/50 [11:46<06:39, 22.19s/batch][A
Batches:  66%|██████▌   | 33/50 [12:08<06:16, 22.18s/batch][A
Batches:  68%|██████▊   | 34/50 [12:30<05:53, 22.12s/batch][A
Batches:  70%|███████   | 35/50 [12:52<05:31, 22.12s/batch][A
Batches:  72%|███████▏  | 36/50 [13:14<05:09, 22.11s/batch][A
Batches:  74%|███████▍  | 37/50 [13:36<04:47, 22.11s/batch][A
Batches:  76%|███████▌  | 38/50 [13:58<04:24, 22.06s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:03, 22.10s/batch][A
Batches:  80%|████████  | 40/50 [14:42<03:40, 22.04s/batch][A
Batches:  82%|████████▏ | 41/50 [15:04<03:18, 22.02s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:56, 22.04s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.07s/batch][A
Batches:  88%|████████▊ | 44/50 [16:11<02:12, 22.07s/batch][A
Batches:  90%|█████████ | 45/50 [16:33<01:50, 22.06s/batch][A
Batches:  92%|█████████▏| 46/50 [16:55<01:28, 22.09s/batch][A
Batches:  94%|█████████▍| 47/50 [17:17<01:06, 22.04s/batch][A
Batches:  96%|█████████▌| 48/50 [17:39<00:44, 22.07s/batch][A
Batches:  98%|█████████▊| 49/50 [18:01<00:22, 22.06s/batch][A
Batches: 100%|██████████| 50/50 [18:23<00:00, 22.11s/batch][A
                                                           [AEpochs:   8%|▊         | 17/200 [5:14:28<56:20:22, 1108.32s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:02, 22.09s/batch][A
Batches:   4%|▍         | 2/50 [00:43<17:34, 21.97s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:15, 22.02s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:55, 22.08s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:33, 22.07s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:13, 22.12s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:50, 22.10s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:30, 22.16s/batch][A
Batches:  18%|█▊        | 9/50 [03:19<15:09, 22.18s/batch][A
Batches:  20%|██        | 10/50 [03:41<14:45, 22.13s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:22, 22.12s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<14:00, 22.12s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:36, 22.07s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:14, 22.08s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:54, 22.12s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:31, 22.10s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:08, 22.08s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:47, 22.12s/batch][A
Batches:  38%|███▊      | 19/50 [07:00<11:25, 22.12s/batch][A
Batches:  40%|████      | 20/50 [07:22<11:03, 22.10s/batch][A
Batches:  42%|████▏     | 21/50 [07:44<10:41, 22.10s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:21, 22.19s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:58, 22.15s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:34, 22.11s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:11, 22.05s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:50, 22.11s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:28, 22.10s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:05, 22.05s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:42, 22.02s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:21, 22.06s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:59, 22.06s/batch][A
Batches:  64%|██████▍   | 32/50 [11:47<06:37, 22.08s/batch][A
Batches:  66%|██████▌   | 33/50 [12:09<06:16, 22.15s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:53, 22.10s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:31, 22.12s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:09, 22.11s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:48, 22.17s/batch][A
Batches:  76%|███████▌  | 38/50 [14:00<04:25, 22.14s/batch][A
Batches:  78%|███████▊  | 39/50 [14:22<04:03, 22.14s/batch][A
Batches:  80%|████████  | 40/50 [14:44<03:41, 22.16s/batch][A
Batches:  82%|████████▏ | 41/50 [15:06<03:19, 22.21s/batch][A
Batches:  84%|████████▍ | 42/50 [15:28<02:57, 22.16s/batch][A
Batches:  86%|████████▌ | 43/50 [15:50<02:34, 22.11s/batch][A
Batches:  88%|████████▊ | 44/50 [16:12<02:12, 22.10s/batch][A
Batches:  90%|█████████ | 45/50 [16:35<01:50, 22.14s/batch][A
Batches:  92%|█████████▏| 46/50 [16:57<01:28, 22.12s/batch][A
Batches:  94%|█████████▍| 47/50 [17:19<01:06, 22.09s/batch][A
Batches:  96%|█████████▌| 48/50 [17:41<00:44, 22.07s/batch][A
Batches:  98%|█████████▊| 49/50 [18:03<00:22, 22.05s/batch][A
Batches: 100%|██████████| 50/50 [18:25<00:00, 22.04s/batch][A
                                                           [AEpochs:   9%|▉         | 18/200 [5:32:57<56:02:50, 1108.63s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:01, 22.07s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:39, 22.07s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:17, 22.07s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:53, 22.04s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:31, 22.04s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:09, 22.03s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:48, 22.06s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:26, 22.05s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:06, 22.11s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:43, 22.08s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:19, 22.04s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:58, 22.08s/batch][A
Batches:  26%|██▌       | 13/50 [04:46<13:36, 22.07s/batch][A
Batches:  28%|██▊       | 14/50 [05:08<13:14, 22.07s/batch][A
Batches:  30%|███       | 15/50 [05:30<12:51, 22.04s/batch][A
Batches:  32%|███▏      | 16/50 [05:52<12:28, 22.02s/batch][A
Batches:  34%|███▍      | 17/50 [06:14<12:07, 22.06s/batch][A
Batches:  36%|███▌      | 18/50 [06:36<11:45, 22.04s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:23, 22.05s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:03, 22.11s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:39, 22.04s/batch][A
Batches:  44%|████▍     | 22/50 [08:05<10:17, 22.04s/batch][A
Batches:  46%|████▌     | 23/50 [08:27<09:55, 22.07s/batch][A
Batches:  48%|████▊     | 24/50 [08:49<09:35, 22.13s/batch][A
Batches:  50%|█████     | 25/50 [09:11<09:12, 22.08s/batch][A
Batches:  52%|█████▏    | 26/50 [09:33<08:49, 22.07s/batch][A
Batches:  54%|█████▍    | 27/50 [09:55<08:27, 22.05s/batch][A
Batches:  56%|█████▌    | 28/50 [10:17<08:05, 22.06s/batch][A
Batches:  58%|█████▊    | 29/50 [10:39<07:43, 22.05s/batch][A
Batches:  60%|██████    | 30/50 [11:01<07:20, 22.05s/batch][A
Batches:  62%|██████▏   | 31/50 [11:23<06:59, 22.06s/batch][A
Batches:  64%|██████▍   | 32/50 [11:45<06:37, 22.08s/batch][A
Batches:  66%|██████▌   | 33/50 [12:07<06:14, 22.03s/batch][A
Batches:  68%|██████▊   | 34/50 [12:29<05:51, 21.97s/batch][A
Batches:  70%|███████   | 35/50 [12:51<05:29, 21.97s/batch][A
Batches:  72%|███████▏  | 36/50 [13:13<05:08, 22.03s/batch][A
Batches:  74%|███████▍  | 37/50 [13:35<04:46, 22.06s/batch][A
Batches:  76%|███████▌  | 38/50 [13:57<04:23, 22.00s/batch][A
Batches:  78%|███████▊  | 39/50 [14:20<04:02, 22.05s/batch][A
Batches:  80%|████████  | 40/50 [14:42<03:40, 22.05s/batch][A
Batches:  82%|████████▏ | 41/50 [15:04<03:18, 22.09s/batch][A
Batches:  84%|████████▍ | 42/50 [15:26<02:57, 22.13s/batch][A
Batches:  86%|████████▌ | 43/50 [15:48<02:34, 22.05s/batch][A
Batches:  88%|████████▊ | 44/50 [16:10<02:12, 22.08s/batch][A
Batches:  90%|█████████ | 45/50 [16:32<01:50, 22.03s/batch][A
Batches:  92%|█████████▏| 46/50 [16:54<01:28, 22.01s/batch][A
Batches:  94%|█████████▍| 47/50 [17:16<01:06, 22.05s/batch][A
Batches:  96%|█████████▌| 48/50 [17:38<00:44, 22.08s/batch][A
Batches:  98%|█████████▊| 49/50 [18:00<00:22, 22.03s/batch][A
Batches: 100%|██████████| 50/50 [18:22<00:00, 22.05s/batch][A
                                                           [AEpochs:  10%|▉         | 19/200 [5:51:24<55:42:34, 1108.04s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:09, 22.24s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:42, 22.13s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:19, 22.12s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:57, 22.11s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:37, 22.17s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:12, 22.10s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:48, 22.06s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:26, 22.07s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:04, 22.05s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:40, 22.02s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:20, 22.07s/batch][A
Batches:  24%|██▍       | 12/50 [04:25<13:59, 22.09s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:36, 22.07s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:13, 22.06s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:52, 22.07s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:32, 22.13s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:09, 22.11s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:48, 22.13s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:25, 22.11s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:02, 22.09s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:40, 22.08s/batch][A
Batches:  44%|████▍     | 22/50 [08:05<10:16, 22.00s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:56, 22.10s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:34, 22.08s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:12, 22.09s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:49, 22.06s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:28, 22.12s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:05, 22.08s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:43, 22.09s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:22, 22.13s/batch][A
Batches:  62%|██████▏   | 31/50 [11:25<07:01, 22.18s/batch][A
Batches:  64%|██████▍   | 32/50 [11:47<06:38, 22.15s/batch][A
Batches:  66%|██████▌   | 33/50 [12:09<06:15, 22.09s/batch][A
Batches:  68%|██████▊   | 34/50 [12:31<05:53, 22.08s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:32, 22.13s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:08, 22.06s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:46, 22.07s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:24, 22.07s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:03, 22.11s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:40, 22.04s/batch][A
Batches:  82%|████████▏ | 41/50 [15:05<03:18, 22.03s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:56, 22.09s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.05s/batch][A
Batches:  88%|████████▊ | 44/50 [16:11<02:12, 22.04s/batch][A
Batches:  90%|█████████ | 45/50 [16:33<01:50, 22.07s/batch][A
Batches:  92%|█████████▏| 46/50 [16:56<01:28, 22.15s/batch][A
Batches:  94%|█████████▍| 47/50 [17:18<01:06, 22.13s/batch][A
Batches:  96%|█████████▌| 48/50 [17:40<00:44, 22.10s/batch][A
Batches:  98%|█████████▊| 49/50 [18:02<00:22, 22.08s/batch][A
Batches: 100%|██████████| 50/50 [18:24<00:00, 22.10s/batch][A
                                                           [AEpochs:  10%|█         | 20/200 [6:09:53<55:24:40, 1108.22s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<17:59, 22.03s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:36, 22.02s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:16, 22.05s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:54, 22.06s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:33, 22.08s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:11, 22.07s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:51, 22.13s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:30, 22.16s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:06, 22.11s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:43, 22.08s/batch][A
Batches:  22%|██▏       | 11/50 [04:03<14:22, 22.12s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:57, 22.05s/batch][A
Batches:  26%|██▌       | 13/50 [04:47<13:36, 22.07s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:13, 22.05s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:53, 22.11s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:30, 22.08s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:08, 22.07s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:46, 22.09s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:25, 22.11s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:02, 22.08s/batch][A
Batches:  42%|████▏     | 21/50 [07:43<10:38, 22.03s/batch][A
Batches:  44%|████▍     | 22/50 [08:05<10:17, 22.07s/batch][A
Batches:  46%|████▌     | 23/50 [08:27<09:56, 22.10s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:34, 22.09s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:12, 22.09s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:50, 22.12s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:29, 22.14s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:06, 22.11s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:43, 22.06s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:21, 22.09s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:59, 22.06s/batch][A
Batches:  64%|██████▍   | 32/50 [11:46<06:36, 22.03s/batch][A
Batches:  66%|██████▌   | 33/50 [12:08<06:15, 22.07s/batch][A
Batches:  68%|██████▊   | 34/50 [12:30<05:52, 22.04s/batch][A
Batches:  70%|███████   | 35/50 [12:52<05:30, 22.06s/batch][A
Batches:  72%|███████▏  | 36/50 [13:14<05:08, 22.01s/batch][A
Batches:  74%|███████▍  | 37/50 [13:36<04:46, 22.02s/batch][A
Batches:  76%|███████▌  | 38/50 [13:58<04:24, 22.08s/batch][A
Batches:  78%|███████▊  | 39/50 [14:20<04:02, 22.06s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:40, 22.06s/batch][A
Batches:  82%|████████▏ | 41/50 [15:05<03:18, 22.10s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:56, 22.07s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.03s/batch][A
Batches:  88%|████████▊ | 44/50 [16:11<02:12, 22.04s/batch][A
Batches:  90%|█████████ | 45/50 [16:33<01:50, 22.11s/batch][A
Batches:  92%|█████████▏| 46/50 [16:55<01:28, 22.11s/batch][A
Batches:  94%|█████████▍| 47/50 [17:17<01:06, 22.07s/batch][A
Batches:  96%|█████████▌| 48/50 [17:39<00:44, 22.05s/batch][A
Batches:  98%|█████████▊| 49/50 [18:01<00:22, 22.09s/batch][A
Batches: 100%|██████████| 50/50 [18:23<00:00, 22.11s/batch][A
                                                           [AEpochs:  10%|█         | 21/200 [6:28:21<55:05:57, 1108.14s/epoch]
Batches:   0%|          | 0/50 [00:00<?, ?batch/s][A/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/grphome/fslg_census/nobackup/archive/envs/torch_2/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

Batches:   2%|▏         | 1/50 [00:22<18:05, 22.15s/batch][A
Batches:   4%|▍         | 2/50 [00:44<17:39, 22.07s/batch][A
Batches:   6%|▌         | 3/50 [01:06<17:18, 22.10s/batch][A
Batches:   8%|▊         | 4/50 [01:28<16:54, 22.05s/batch][A
Batches:  10%|█         | 5/50 [01:50<16:30, 22.01s/batch][A
Batches:  12%|█▏        | 6/50 [02:12<16:12, 22.10s/batch][A
Batches:  14%|█▍        | 7/50 [02:34<15:50, 22.11s/batch][A
Batches:  16%|█▌        | 8/50 [02:56<15:25, 22.05s/batch][A
Batches:  18%|█▊        | 9/50 [03:18<15:04, 22.06s/batch][A
Batches:  20%|██        | 10/50 [03:40<14:42, 22.06s/batch][A
Batches:  22%|██▏       | 11/50 [04:02<14:21, 22.08s/batch][A
Batches:  24%|██▍       | 12/50 [04:24<13:59, 22.09s/batch][A
Batches:  26%|██▌       | 13/50 [04:46<13:36, 22.07s/batch][A
Batches:  28%|██▊       | 14/50 [05:09<13:15, 22.11s/batch][A
Batches:  30%|███       | 15/50 [05:31<12:53, 22.11s/batch][A
Batches:  32%|███▏      | 16/50 [05:53<12:30, 22.09s/batch][A
Batches:  34%|███▍      | 17/50 [06:15<12:08, 22.08s/batch][A
Batches:  36%|███▌      | 18/50 [06:37<11:48, 22.14s/batch][A
Batches:  38%|███▊      | 19/50 [06:59<11:25, 22.13s/batch][A
Batches:  40%|████      | 20/50 [07:21<11:03, 22.12s/batch][A
Batches:  42%|████▏     | 21/50 [07:44<10:42, 22.15s/batch][A
Batches:  44%|████▍     | 22/50 [08:06<10:21, 22.19s/batch][A
Batches:  46%|████▌     | 23/50 [08:28<09:56, 22.10s/batch][A
Batches:  48%|████▊     | 24/50 [08:50<09:33, 22.05s/batch][A
Batches:  50%|█████     | 25/50 [09:12<09:12, 22.10s/batch][A
Batches:  52%|█████▏    | 26/50 [09:34<08:49, 22.07s/batch][A
Batches:  54%|█████▍    | 27/50 [09:56<08:26, 22.02s/batch][A
Batches:  56%|█████▌    | 28/50 [10:18<08:06, 22.11s/batch][A
Batches:  58%|█████▊    | 29/50 [10:40<07:44, 22.10s/batch][A
Batches:  60%|██████    | 30/50 [11:02<07:21, 22.06s/batch][A
Batches:  62%|██████▏   | 31/50 [11:24<06:59, 22.06s/batch][A
Batches:  64%|██████▍   | 32/50 [11:46<06:37, 22.09s/batch][A
Batches:  66%|██████▌   | 33/50 [12:08<06:15, 22.09s/batch][A
Batches:  68%|██████▊   | 34/50 [12:30<05:53, 22.08s/batch][A
Batches:  70%|███████   | 35/50 [12:53<05:30, 22.06s/batch][A
Batches:  72%|███████▏  | 36/50 [13:15<05:09, 22.11s/batch][A
Batches:  74%|███████▍  | 37/50 [13:37<04:47, 22.09s/batch][A
Batches:  76%|███████▌  | 38/50 [13:59<04:25, 22.09s/batch][A
Batches:  78%|███████▊  | 39/50 [14:21<04:03, 22.10s/batch][A
Batches:  80%|████████  | 40/50 [14:43<03:40, 22.10s/batch][A
Batches:  82%|████████▏ | 41/50 [15:05<03:18, 22.02s/batch][A
Batches:  84%|████████▍ | 42/50 [15:27<02:56, 22.02s/batch][A
Batches:  86%|████████▌ | 43/50 [15:49<02:34, 22.11s/batch][A
Batches:  88%|████████▊ | 44/50 [16:11<02:12, 22.08s/batch][A
Batches:  90%|█████████ | 45/50 [16:33<01:50, 22.07s/batch][A
Batches:  92%|█████████▏| 46/50 [16:55<01:28, 22.06s/batch][A
Batches:  94%|█████████▍| 47/50 [17:18<01:06, 22.09s/batch][A
Batches:  96%|█████████▌| 48/50 [17:40<00:44, 22.07s/batch][A
Batches:  98%|█████████▊| 49/50 [18:02<00:22, 22.09s/batch][A
Batches: 100%|██████████| 50/50 [18:24<00:00, 22.08s/batch][A
                                                           [AEpochs:  10%|█         | 21/200 [6:46:45<57:47:05, 1162.16s/epoch]
